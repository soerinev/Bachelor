{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2766135f-bd17-4379-a22b-bcc860f332fa",
   "metadata": {},
   "source": [
    "# Purpose of notebook: playground for doing word embeddings + semantic polarity\n",
    "\n",
    "# Where we at: BERT virker, SBERT vil ikke importeres. Er bange for conda\n",
    "\n",
    "# 沐･ = useful chunks fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ebd09d-50b4-4abd-879a-17016c24e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m148.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m154.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (362 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow, tf-keras\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 h5py-3.12.1 keras-3.6.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.13.0 protobuf-5.28.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 tf-keras-2.18.0 werkzeug-3.1.3 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.12/site-packages (from scipy) (2.0.2)\n",
      "Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 sentence-transformers-3.3.0 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# libraries沐･\n",
    "\n",
    "# install\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install tf-keras # install this because keras needs to work on a previous version of python\n",
    "%pip install scipy\n",
    "%pip install pandas\n",
    "%pip install tensorflow\n",
    "%pip install -U sentence-transformers\n",
    "%pip install numpy\n",
    "\n",
    "\n",
    "\n",
    "# import\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46971c74-da0f-4852-b1b4-214464328364",
   "metadata": {},
   "source": [
    "## test playground for 1 sentence with BERT (bert-base-uncased) - works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645d783c-1758-40ce-8d35-c5fb60d2e8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0988d560ef4f6c9cccd71893435be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b454ce6af948638791fb84aa976078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca110a0467dd42b7b88c10bd06d1de31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db84421a134144e68cc97d6f44ff729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a959561b4da4d3a9808b7d4e8c08fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokenizer and model (uncased means it converts all text to lowercase and removes any accent markers before tokenization)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3675f9-9312-4df2-868e-50b14d46a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the input text and convert it into token IDs and attention masks (Token IDs represent the input tokens in numeric form that BERT can process. Attention Masks tell the model which tokens are real words and which are padding (for aligning input lengths))\n",
    "text = \"Abortion is murder of innocent babies.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf') # This tells the tokenizer to return the tokenized output as TensorFlow tensors (which are the required format for TFBertModel). So the returned encoded_input contains input_ids: Numerical IDs for the tokens and attention_mask: A binary mask indicating which positions are padding (0) or actual data (1) (this is important when dealing with more text where sentences or document differ in length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84663cba-5338-4cbe-944f-716a28b697f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the Tokenized Input Through the BERT Model\n",
    "output = model(encoded_input)\n",
    "#output # makes no sense to the human eye but I think it works as it should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1c421-d101-445c-b2df-b8e6b34b6ba4",
   "metadata": {},
   "source": [
    "## test playground for 2 sentences text embedding and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8c826-9cf1-4c8e-8393-4a3bbed81dd0",
   "metadata": {},
   "source": [
    "#### Word level - works not very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc91c57-d9aa-4bdb-af9d-84d64be59de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity for 'abortion': 0.9759\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to get contextual embedding for a specific word\n",
    "def get_word_embedding(text, target_word):\n",
    "    encoded_input = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "    outputs = model(encoded_input)\n",
    "    \n",
    "    # Extract token embeddings\n",
    "    token_embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "    \n",
    "    # Find the token index for the target word\n",
    "    token_ids = encoded_input['input_ids'][0]\n",
    "    target_word_id = tokenizer.convert_tokens_to_ids(target_word)\n",
    "    word_indices = tf.where(token_ids == target_word_id)\n",
    "    \n",
    "    if len(word_indices) == 0:\n",
    "        print(f\"'{target_word}' not found in the sentence.\")\n",
    "        return None\n",
    "    \n",
    "    # Average embeddings if the word appears multiple times\n",
    "    word_embedding = tf.reduce_mean(tf.gather(token_embeddings[0], word_indices[:, 0]), axis=0)\n",
    "    return word_embedding\n",
    "\n",
    "# First sentence\n",
    "sentence1 = \"Abortion is murder of babies.\"\n",
    "embedding1 = get_word_embedding(sentence1, 'abortion')\n",
    "\n",
    "# Second sentence\n",
    "sentence2 = \"Abortion is a safe medical procedure for women.\"\n",
    "embedding2 = get_word_embedding(sentence2, 'abortion')\n",
    "\n",
    "# Check if both embeddings are valid\n",
    "if embedding1 is not None and embedding2 is not None:\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = 1 - cosine(embedding1.numpy(), embedding2.numpy())\n",
    "    print(f\"Semantic Similarity for 'abortion': {similarity_score:.4f}\")\n",
    "else:\n",
    "    print(\"Failed to compute similarity due to missing embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a3c8d7-e706-4f26-88f9-d8c76e618aae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_word_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Sentence 1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbortion is reproductive health care.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m embedding1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_word_embedding\u001b[49m(sentence1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabortion\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Sentence 2\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sentence2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbortion is a safe medical procedure for women.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_word_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "def get_sentence_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "    output = model(encoded_input)\n",
    "    # Use [CLS] token embedding for the entire sentence\n",
    "    cls_embedding = output.last_hidden_state[:, 0, :]  # [CLS] token is at index 0\n",
    "    return cls_embedding\n",
    "\n",
    "# Sentence 1\n",
    "sentence1 = \"Abortion is reproductive health care.\"\n",
    "embedding1 = get_word_embedding(sentence1, 'abortion')\n",
    "\n",
    "# Sentence 2\n",
    "sentence2 = \"Abortion is a safe medical procedure for women.\"\n",
    "embedding2 = get_word_embedding(sentence2, 'abortion')\n",
    "\n",
    "# Compute embeddings for entire sentences\n",
    "embedding1 = get_sentence_embedding(sentence1)\n",
    "embedding2 = get_sentence_embedding(sentence2)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_score = 1 - cosine(embedding1.numpy(), embedding2.numpy())\n",
    "print(f\"Semantic Similarity for entire sentences: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d30464-e131-48af-953f-e339d0577b70",
   "metadata": {},
   "source": [
    "### SentenceTransformer model - maybe not appropriate for here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23e8d412-cacb-4c00-9a2d-efe10d33051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity using SBERT: 0.8555\n"
     ]
    }
   ],
   "source": [
    "# SBERT\n",
    "\n",
    "# Load pretrained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and effective model\n",
    "\n",
    "# Sentences to compare\n",
    "sentence1 = \"Abortion is reproductive health care.\"\n",
    "sentence2 = \"Abortion is a human health care right.\"\n",
    "\n",
    "# Get sentence embeddings\n",
    "embedding1 = model.encode(sentence1)\n",
    "embedding2 = model.encode(sentence2)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_score = 1 - cosine(embedding1, embedding2)\n",
    "print(f\"Semantic Similarity using SBERT: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b423988-5a33-4457-b8b3-8501bcc3500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n",
      "tensor([[1.0000, 0.7141, 0.6337],\n",
      "        [0.7141, 1.0000, 0.6166],\n",
      "        [0.6337, 0.6166, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and effective model\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"Abortion is a human right\",\n",
    "    \"Abortion is murder on innocent babies\",\n",
    "    \"Abortion should be banned in all states\",\n",
    "]\n",
    "\n",
    "# Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "\n",
    "# Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03264d-b91e-4931-b5a4-c2b5a5d4ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# List of sentences to be processed\n",
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n",
    "\n",
    "# Initializing the Sentence Transformer model using BERT with mean-tokens pooling\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Encoding the sentences to obtain their embeddings\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# Calculating the cosine similarity between the first sentence embedding and the rest of the embeddings\n",
    "# The result will be a list of similarity scores between the first sentence and each of the other sentences\n",
    "similarity_scores = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
    "similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145904e9-a821-4b61-97c8-2639d796ec92",
   "metadata": {},
   "source": [
    "### Token-based SentenceTransformer model - verdict tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9797529d-e361-4906-9042-5a3cf13207b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "11328 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     word_idx \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;241m.\u001b[39mindex(model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(target_word))\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_embeddings[\u001b[38;5;241m0\u001b[39m][word_idx]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 23\u001b[0m embedding1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_embeddings1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabortion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m embedding2 \u001b[38;5;241m=\u001b[39m get_token_embedding(tokens2, token_embeddings2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabortion\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mget_token_embedding\u001b[0;34m(tokens, token_embeddings, target_word)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_embedding\u001b[39m(tokens, token_embeddings, target_word):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Identify the token index for the word 'abortion'\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     word_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_embeddings[\u001b[38;5;241m0\u001b[39m][word_idx]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mValueError\u001b[0m: 11328 is not in list"
     ]
    }
   ],
   "source": [
    "# Load a Token-Based SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')  # Example model\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"Abortion is a controversial topic in politics.\"\n",
    "sentence2 = \"Abortion is a medical procedure.\"\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "tokens1 = model.tokenize(sentence1)\n",
    "tokens2 = model.tokenize(sentence2)\n",
    "\n",
    "# Get token embeddings for each sentence\n",
    "with torch.no_grad():\n",
    "    token_embeddings1 = model(tokens1)['token_embeddings']\n",
    "    token_embeddings2 = model(tokens2)['token_embeddings']\n",
    "\n",
    "# Find token embeddings for the word \"abortion\"\n",
    "def get_token_embedding(tokens, token_embeddings, target_word):\n",
    "    # Identify the token index for the word 'abortion'\n",
    "    word_idx = tokens['input_ids'][0].tolist().index(model.tokenizer.convert_tokens_to_ids(target_word))\n",
    "    return token_embeddings[0][word_idx].detach().numpy()\n",
    "\n",
    "embedding1 = get_token_embedding(tokens1, token_embeddings1, 'abortion')\n",
    "embedding2 = get_token_embedding(tokens2, token_embeddings2, 'abortion')\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = 1 - cosine(embedding1, embedding2)\n",
    "print(f\"Cosine similarity for 'abortion': {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba5ee678-35ab-42f2-9b12-9b14167428ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'abortion' or its subwords not found in the tokenized sentence.\n",
      "'abortion' or its subwords not found in the tokenized sentence.\n"
     ]
    }
   ],
   "source": [
    "# Load a Token-Based SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"Abortion is a controversial topic in politics.\"\n",
    "sentence2 = \"Abortion is a medical procedure.\"\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "tokens1 = model.tokenize(sentence1)\n",
    "tokens2 = model.tokenize(sentence2)\n",
    "\n",
    "# Get token embeddings for each sentence\n",
    "with torch.no_grad():\n",
    "    token_embeddings1 = model(tokens1)['token_embeddings']\n",
    "    token_embeddings2 = model(tokens2)['token_embeddings']\n",
    "\n",
    "# Updated function to handle subword tokenization\n",
    "def get_token_embedding(tokens, token_embeddings, target_word):\n",
    "    # Get the tokenized word pieces for the target word\n",
    "    target_word_id = model.tokenizer.convert_tokens_to_ids(target_word)\n",
    "    \n",
    "    # Find all token indices for subword parts of the target word\n",
    "    word_indices = [\n",
    "        i for i, token_id in enumerate(tokens['input_ids'][0].tolist())\n",
    "        if token_id == target_word_id\n",
    "    ]\n",
    "\n",
    "    if not word_indices:\n",
    "        print(f\"'{target_word}' or its subwords not found in the tokenized sentence.\")\n",
    "        return None\n",
    "    \n",
    "    # Average embeddings if multiple subwords\n",
    "    word_embedding = token_embeddings[0][word_indices].mean(dim=0).detach().numpy()\n",
    "    return word_embedding\n",
    "\n",
    "# Find token embeddings for the word \"abortion\"\n",
    "embedding1 = get_token_embedding(tokens1, token_embeddings1, 'abortion')\n",
    "embedding2 = get_token_embedding(tokens2, token_embeddings2, 'abortion')\n",
    "\n",
    "# Calculate cosine similarity\n",
    "if embedding1 is not None and embedding2 is not None:\n",
    "    similarity = 1 - cosine(embedding1, embedding2)\n",
    "    print(f\"Cosine similarity for 'abortion': {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd8566c-d91e-4908-b7e6-2b656f727b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'abortion' or its subwords not found.\n",
      "'abortion' or its subwords not found.\n"
     ]
    }
   ],
   "source": [
    "# Load a Token-Based SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"Abortion is a controversial topic in politics.\"\n",
    "sentence2 = \"Abortion is a medical procedure.\"\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "tokens1 = model.tokenize(sentence1)\n",
    "tokens2 = model.tokenize(sentence2)\n",
    "\n",
    "# Get token embeddings for each sentence\n",
    "with torch.no_grad():\n",
    "    token_embeddings1 = model(tokens1)['token_embeddings']\n",
    "    token_embeddings2 = model(tokens2)['token_embeddings']\n",
    "\n",
    "# Updated function to handle subword tokenization\n",
    "def get_token_embedding(tokens, token_embeddings, target_word):\n",
    "    subword_ids = model.tokenizer(target_word, add_special_tokens=False)['input_ids']\n",
    "    sentence_ids = tokens['input_ids'][0].tolist()\n",
    "    \n",
    "    for i in range(len(sentence_ids) - len(subword_ids) + 1):\n",
    "        if sentence_ids[i:i + len(subword_ids)] == subword_ids:\n",
    "            word_embedding = token_embeddings[0][i:i + len(subword_ids)].mean(dim=0).detach().numpy()\n",
    "            return word_embedding\n",
    "    \n",
    "    print(f\"'{target_word}' or its subwords not found.\")\n",
    "    return None\n",
    "\n",
    "# Find token embeddings for the word \"abortion\"\n",
    "embedding1 = get_token_embedding(tokens1, token_embeddings1, 'abortion')\n",
    "embedding2 = get_token_embedding(tokens2, token_embeddings2, 'abortion')\n",
    "\n",
    "# Calculate cosine similarity\n",
    "if embedding1 is not None and embedding2 is not None:\n",
    "    similarity = 1 - cosine(embedding1, embedding2)\n",
    "    print(f\"Cosine similarity for 'abortion': {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a7848f7-8991-420e-84cc-d0580336de73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33088905, 0.72192585, 0.5548365 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176327da-ed88-4ede-adbc-f69c33474e6d",
   "metadata": {},
   "source": [
    "## Keyword specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f323d6d-f071-46fc-86d6-803eb77072d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2179919-9c45-4ad9-b7da-26968703ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'abortion' and Article 1: 0.4611\n",
      "Similarity between 'abortion' and Article 2: 0.5539\n",
      "Similarity between 'abortion' and Article 3: 0.5868\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    # Take the mean of the last hidden state (average pooling)\n",
    "    embedding = output.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embedding.numpy()\n",
    "\n",
    "# Step 2: Embed articles\n",
    "articles = [\n",
    "    \"Abortion is a highly controversial topic in today's society.\",\n",
    "    \"Healthcare providers offer abortion services to women in need.\",\n",
    "    \"Some laws heavily restrict abortion access.\"\n",
    "]\n",
    "\n",
    "article_embeddings = [get_embedding(article) for article in articles]\n",
    "\n",
    "# Step 3: Embed the keyword\n",
    "keyword = \"abortion\"\n",
    "keyword_embedding = get_embedding(keyword).reshape(1, -1)\n",
    "\n",
    "# Step 4: Calculate semantic similarity\n",
    "similarities = [cosine_similarity(keyword_embedding, embedding.reshape(1, -1))[0][0] for embedding in article_embeddings]\n",
    "\n",
    "# Step 5: Display results\n",
    "for i, similarity in enumerate(similarities):\n",
    "    print(f\"Similarity between '{keyword}' and Article {i+1}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed2aa90-d5af-4e0d-a22c-16169b9e388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Similarities:\n",
      "Article 1: 0.4611\n",
      "Article 2: 0.5539\n",
      "\n",
      "Fox News Similarities:\n",
      "Article 1: 0.5868\n",
      "Article 2: 0.6444\n",
      "\n",
      "Average Similarity for CNN: 0.5075\n",
      "Average Similarity for Fox News: 0.6156\n"
     ]
    }
   ],
   "source": [
    "# with CNN and Fox articles\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    embedding = output.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embedding.numpy()\n",
    "\n",
    "# CNN and Fox News articles\n",
    "cnn_articles = [\n",
    "    \"Abortion is a highly controversial topic in today's society.\",\n",
    "    \"Healthcare providers offer abortion services to women in need.\"\n",
    "]\n",
    "\n",
    "fox_articles = [\n",
    "    \"Some laws heavily restrict abortion access.\",\n",
    "    \"Abortion is banned many places.\"\n",
    "]\n",
    "\n",
    "# Embed articles\n",
    "cnn_embeddings = [get_embedding(article) for article in cnn_articles]\n",
    "fox_embeddings = [get_embedding(article) for article in fox_articles]\n",
    "\n",
    "# Embed the keyword\n",
    "keyword = \"abortion\"\n",
    "keyword_embedding = get_embedding(keyword).reshape(1, -1)\n",
    "\n",
    "# Calculate semantic similarity for each group\n",
    "cnn_similarities = [cosine_similarity(keyword_embedding, embedding.reshape(1, -1))[0][0] for embedding in cnn_embeddings]\n",
    "fox_similarities = [cosine_similarity(keyword_embedding, embedding.reshape(1, -1))[0][0] for embedding in fox_embeddings]\n",
    "\n",
    "# Display results\n",
    "print(\"CNN Similarities:\")\n",
    "for i, similarity in enumerate(cnn_similarities):\n",
    "    print(f\"Article {i+1}: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\nFox News Similarities:\")\n",
    "for i, similarity in enumerate(fox_similarities):\n",
    "    print(f\"Article {i+1}: {similarity:.4f}\")\n",
    "\n",
    "# Calculate average similarity for each group\n",
    "avg_cnn_similarity = np.mean(cnn_similarities)\n",
    "avg_fox_similarity = np.mean(fox_similarities)\n",
    "\n",
    "print(f\"\\nAverage Similarity for CNN: {avg_cnn_similarity:.4f}\")\n",
    "print(f\"Average Similarity for Fox News: {avg_fox_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c599744a-249b-41f8-8caa-21557330c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Similarities between CNN and Fox News contextual embeddings for 'abortion':\n",
      "Pair 1: 0.9636\n",
      "Pair 2: 0.2296\n",
      "Pair 3: 0.9687\n",
      "Pair 4: 0.1585\n",
      "\n",
      "Average Similarity between CNN and Fox News usage of 'abortion': 0.5801\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get contextual embedding for a specific word in a text\n",
    "def get_contextual_embedding(text, keyword):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    # Get the token index for the keyword\n",
    "    keyword_index = (tokens.input_ids == tokenizer.encode(keyword, add_special_tokens=False)[0]).nonzero(as_tuple=True)[1]\n",
    "    # Extract the embedding for the keyword in its context\n",
    "    keyword_embedding = output.last_hidden_state[0, keyword_index, :].mean(dim=0)\n",
    "    return keyword_embedding.numpy()\n",
    "\n",
    "# CNN and Fox News articles\n",
    "cnn_articles = [\n",
    "    \"Abortion is murder\",\n",
    "    \"Abortion is health care\"\n",
    "]\n",
    "\n",
    "fox_articles = [\n",
    "    \"Abortion is killing babies\",\n",
    "    \"Healthcare providers offer abortion services to women in need.\"\n",
    "]\n",
    "\n",
    "# Get contextual embeddings for 'abortion' in each article\n",
    "cnn_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in cnn_articles]\n",
    "fox_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in fox_articles]\n",
    "\n",
    "# Calculate pairwise similarities between CNN and Fox News contextual embeddings\n",
    "similarities = []\n",
    "for cnn_embedding in cnn_contextual_embeddings:\n",
    "    for fox_embedding in fox_contextual_embeddings:\n",
    "        similarity = cosine_similarity(cnn_embedding.reshape(1, -1), fox_embedding.reshape(1, -1))[0][0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "# Display results\n",
    "print(\"Pairwise Similarities between CNN and Fox News contextual embeddings for 'abortion':\")\n",
    "for i, similarity in enumerate(similarities):\n",
    "    print(f\"Pair {i+1}: {similarity:.4f}\")\n",
    "\n",
    "# Average similarity across CNN vs Fox contexts\n",
    "avg_similarity = np.mean(similarities)\n",
    "print(f\"\\nAverage Similarity between CNN and Fox News usage of 'abortion': {avg_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "493e3aca-4e3e-4d18-b210-0306bb1308a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between CNN and Fox News aggregated usage of 'abortion': 0.9342\n"
     ]
    }
   ],
   "source": [
    "# PYTORCH\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get contextual embedding for a specific word in a text\n",
    "def get_contextual_embedding(text, keyword):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    # Get the token index for the keyword\n",
    "    keyword_index = (tokens.input_ids == tokenizer.encode(keyword, add_special_tokens=False)[0]).nonzero(as_tuple=True)[1]\n",
    "    # Extract the embedding for the keyword in its context\n",
    "    keyword_embedding = output.last_hidden_state[0, keyword_index, :].mean(dim=0)\n",
    "    return keyword_embedding.numpy()\n",
    "\n",
    "# CNN and Fox News articles\n",
    "cnn_articles = [\n",
    "    \"Abortion is murder.\",\n",
    "    \"Healthcare providers should not offer abortion services to women in need.\"\n",
    "]\n",
    "\n",
    "fox_articles = [\n",
    "    \"Some laws heavily restrict abortion access, and this is taking away women's rights.\",\n",
    "    \"Abortion should be legal in all states.\"\n",
    "]\n",
    "\n",
    "# Get contextual embeddings for 'abortion' in each article\n",
    "cnn_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in cnn_articles]\n",
    "fox_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in fox_articles]\n",
    "\n",
    "# Step 1: Aggregate embeddings within each source\n",
    "cnn_aggregated_embedding = np.mean(cnn_contextual_embeddings, axis=0)\n",
    "fox_aggregated_embedding = np.mean(fox_contextual_embeddings, axis=0)\n",
    "\n",
    "# Step 2: Compare aggregated embeddings between CNN and Fox News\n",
    "similarity_between_sources = cosine_similarity(\n",
    "    cnn_aggregated_embedding.reshape(1, -1), \n",
    "    fox_aggregated_embedding.reshape(1, -1)\n",
    ")[0][0]\n",
    "\n",
    "# Display results\n",
    "print(f\"Similarity between CNN and Fox News aggregated usage of 'abortion': {similarity_between_sources:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72118be6-d6fa-40ab-80d3-32762861d7fc",
   "metadata": {},
   "source": [
    "### Using pytorch or tensorflow for semantic similarity gives the exact same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e679a88-d019-4d83-9f1d-e0e377b9306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no_gradient() missing 1 required positional argument: 'op_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m fox_articles \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome laws heavily restrict abortion access, and this is taking away women\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms rights.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbortion should be legal in all states.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Get contextual embeddings for 'abortion' in each article\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m cnn_contextual_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mget_contextual_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabortion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m cnn_articles]\n\u001b[1;32m     32\u001b[0m fox_contextual_embeddings \u001b[38;5;241m=\u001b[39m [get_contextual_embedding(article, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabortion\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m fox_articles]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Step 1: Aggregate embeddings within each source\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m, in \u001b[0;36mget_contextual_embedding\u001b[0;34m(text, keyword)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_contextual_embedding\u001b[39m(text, keyword):\n\u001b[1;32m      9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     11\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Find the token index of the keyword\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: no_gradient() missing 1 required positional argument: 'op_type'"
     ]
    }
   ],
   "source": [
    "# TENSORFLOW\n",
    "\n",
    "# Load pre-trained TensorFlow BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get contextual embedding for a specific word in a text\n",
    "def get_contextual_embedding(text, keyword):\n",
    "    tokens = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "    with tf.no_gradient():\n",
    "        output = model(**tokens)\n",
    "    # Find the token index of the keyword\n",
    "    keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "    keyword_index = tf.where(tokens.input_ids == keyword_id)[0][1]  # First occurrence index\n",
    "    # Extract contextual embedding\n",
    "    keyword_embedding = tf.reduce_mean(output.last_hidden_state[:, keyword_index, :], axis=1)\n",
    "    return keyword_embedding.numpy()\n",
    "\n",
    "# CNN and Fox News articles\n",
    "cnn_articles = [\n",
    "    \"Abortion is murder.\",\n",
    "    \"Healthcare providers should not offer abortion services to women in need.\"\n",
    "]\n",
    "\n",
    "fox_articles = [\n",
    "    \"Some laws heavily restrict abortion access, and this is taking away women's rights.\",\n",
    "    \"Abortion should be legal in all states.\"\n",
    "]\n",
    "\n",
    "# Get contextual embeddings for 'abortion' in each article\n",
    "cnn_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in cnn_articles]\n",
    "fox_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in fox_articles]\n",
    "\n",
    "# Step 1: Aggregate embeddings within each source\n",
    "cnn_aggregated_embedding = np.mean(cnn_contextual_embeddings, axis=0)\n",
    "fox_aggregated_embedding = np.mean(fox_contextual_embeddings, axis=0)\n",
    "\n",
    "# Step 2: Compare aggregated embeddings between CNN and Fox News\n",
    "similarity_between_sources = cosine_similarity(\n",
    "    cnn_aggregated_embedding.reshape(1, -1), \n",
    "    fox_aggregated_embedding.reshape(1, -1)\n",
    ")[0][0]\n",
    "\n",
    "# Display results\n",
    "print(f\"Similarity between CNN and Fox News aggregated usage of 'abortion': {similarity_between_sources:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d655ed54-1c46-482b-825e-df600a858109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between CNN and Fox News aggregated usage of 'abortion': 0.9342\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained TensorFlow BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get contextual embedding for a specific word in a text\n",
    "def get_contextual_embedding(text, keyword):\n",
    "    tokens = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "    output = model(**tokens)\n",
    "    \n",
    "    # Find the token index of the keyword\n",
    "    keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "    keyword_index = tf.where(tokens.input_ids == keyword_id)[0][1]  # First occurrence index\n",
    "    \n",
    "    # Extract contextual embedding for the keyword\n",
    "    keyword_embedding = tf.reduce_mean(output.last_hidden_state[:, keyword_index, :], axis=0)\n",
    "    return keyword_embedding.numpy()\n",
    "\n",
    "# CNN and Fox News articles\n",
    "cnn_articles = [\n",
    "    \"Abortion is murder.\",\n",
    "    \"Healthcare providers should not offer abortion services to women in need.\"\n",
    "]\n",
    "\n",
    "fox_articles = [\n",
    "    \"Some laws heavily restrict abortion access, and this is taking away women's rights.\",\n",
    "    \"Abortion should be legal in all states.\"\n",
    "]\n",
    "\n",
    "# Get contextual embeddings for 'abortion' in each article\n",
    "cnn_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in cnn_articles]\n",
    "fox_contextual_embeddings = [get_contextual_embedding(article, 'abortion') for article in fox_articles]\n",
    "\n",
    "# Step 1: Aggregate embeddings within each source\n",
    "cnn_aggregated_embedding = np.mean(cnn_contextual_embeddings, axis=0)\n",
    "fox_aggregated_embedding = np.mean(fox_contextual_embeddings, axis=0)\n",
    "\n",
    "# Step 2: Compare aggregated embeddings between CNN and Fox News\n",
    "similarity_between_sources = cosine_similarity(\n",
    "    cnn_aggregated_embedding.reshape(1, -1), \n",
    "    fox_aggregated_embedding.reshape(1, -1)\n",
    ")[0][0]\n",
    "\n",
    "# Display results\n",
    "print(f\"Similarity between CNN and Fox News aggregated usage of 'abortion': {similarity_between_sources:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b6a6a-9bc2-4736-b224-0e601be9c5fa",
   "metadata": {},
   "source": [
    "## trying to do **semantic polarity** like Ding et al. (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de57d79c-3e00-4f64-96d8-0d45eb788641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Polarity (SP) between CNN and Fox News for 'abortion': 0.4481\n"
     ]
    }
   ],
   "source": [
    "# using pytorch not tensorflow\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get contextual embedding for a specific word in a text\n",
    "def get_contextual_embeddings(texts, keyword):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            output = model(**tokens)\n",
    "        # Get the token index for the keyword\n",
    "        keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "        keyword_index = (tokens.input_ids == keyword_id).nonzero(as_tuple=True)[1]\n",
    "        \n",
    "        if len(keyword_index) > 0:  # Only if the keyword exists in the text\n",
    "            keyword_embedding = output.last_hidden_state[0, keyword_index, :].mean(dim=0)\n",
    "            embeddings.append(keyword_embedding.numpy())\n",
    "    return embeddings\n",
    "\n",
    "# CNN and Fox News articles\n",
    "cnn_articles = [\n",
    "    \"Abortion is murder.\",\n",
    "    \"Healthcare providers should not offer abortion services to women in need.\"\n",
    "]\n",
    "\n",
    "fox_articles = [\n",
    "    \"Some laws heavily restrict abortion access, and this is taking away women's rights.\",\n",
    "    \"Abortion should be legal in all states.\"\n",
    "]\n",
    "\n",
    "# Get contextual embeddings for 'abortion' from CNN and Fox News\n",
    "cnn_embeddings = get_contextual_embeddings(cnn_articles, 'abortion')\n",
    "fox_embeddings = get_contextual_embeddings(fox_articles, 'abortion')\n",
    "\n",
    "# Compute Semantic Polarity (SP) between CNN and Fox News\n",
    "def compute_semantic_polarity(embeddings1, embeddings2):\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    for emb1 in embeddings1:\n",
    "        for emb2 in embeddings2:\n",
    "            distance = 1 - cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "semantic_polarity = compute_semantic_polarity(cnn_embeddings, fox_embeddings)\n",
    "\n",
    "# Display results\n",
    "print(f\"Semantic Polarity (SP) between CNN and Fox News for 'abortion': {semantic_polarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b71b7f-0c59-4ba9-955e-7264241280f7",
   "metadata": {},
   "source": [
    "### Code works, might work... Testing over time to see if that works and to see how good this whole thing is at measuring semantic polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f17a1-8bd7-4aa3-b133-1626ea32b34e",
   "metadata": {},
   "source": [
    "#### Make test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb7d44a3-b267-4776-9b06-281edee207bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>words</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12345678</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example1.com</td>\n",
       "      <td>Title for article 1</td>\n",
       "      <td>In today窶冱 society, the importance of bodily a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87654321</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example2.com</td>\n",
       "      <td>Title for article 2</td>\n",
       "      <td>Abortion is not okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23456789</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example3.com</td>\n",
       "      <td>Title for article 3</td>\n",
       "      <td>Abortion is good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98765432</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-10</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example4.com</td>\n",
       "      <td>Title for article 4</td>\n",
       "      <td>Abortion is bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34567890</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example5.com</td>\n",
       "      <td>Title for article 5</td>\n",
       "      <td>Abortion is something I am for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76543210</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example6.com</td>\n",
       "      <td>Title for article 6</td>\n",
       "      <td>Abortion is something I am against.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45678901</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-29</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example7.com</td>\n",
       "      <td>Title for article 7</td>\n",
       "      <td>Abortion is cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10987654</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-05-12</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example8.com</td>\n",
       "      <td>Title for article 8</td>\n",
       "      <td>Abortion is uncool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56789012</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-09-25</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example9.com</td>\n",
       "      <td>Title for article 9</td>\n",
       "      <td>Abortion is definitely nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21098765</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example10.com</td>\n",
       "      <td>Title for article 10</td>\n",
       "      <td>Abortion is definitely not nice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>67890123</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example11.com</td>\n",
       "      <td>Title for article 11</td>\n",
       "      <td>Abortion is the best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32109876</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example12.com</td>\n",
       "      <td>Title for article 12</td>\n",
       "      <td>Abortion is the worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>78901234</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-07-05</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example13.com</td>\n",
       "      <td>Title for article 13</td>\n",
       "      <td>I love Abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43210987</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-10-08</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example14.com</td>\n",
       "      <td>Title for article 14</td>\n",
       "      <td>I hate Abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>89012345</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-12-13</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example15.com</td>\n",
       "      <td>Title for article 15</td>\n",
       "      <td>Abortion is best thing ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>54321098</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example16.com</td>\n",
       "      <td>Title for article 16</td>\n",
       "      <td>Abortion is murder against innocent babies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>90123456</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example17.com</td>\n",
       "      <td>Title for article 17</td>\n",
       "      <td>Abortion is a cornerstone of women's rights.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>65432109</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-08-25</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example18.com</td>\n",
       "      <td>Title for article 18</td>\n",
       "      <td>Abortion should be banned in all states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11223344</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>US</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://example19.com</td>\n",
       "      <td>Title for article 19</td>\n",
       "      <td>In today窶冱 society, the importance of bodily a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44332211</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-11-05</td>\n",
       "      <td>US</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>http://example20.com</td>\n",
       "      <td>Title for article 20</td>\n",
       "      <td>Abortion is unjust and takes away innocent lives.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      textID  words        date country    source                   url  \\\n",
       "0   12345678      5  2020-11-03      US       CNN   http://example1.com   \n",
       "1   87654321      5  2020-12-15      US  Fox News   http://example2.com   \n",
       "2   23456789      5  2021-03-22      US       CNN   http://example3.com   \n",
       "3   98765432      5  2021-05-10      US  Fox News   http://example4.com   \n",
       "4   34567890      5  2021-10-05      US       CNN   http://example5.com   \n",
       "5   76543210      5  2021-12-18      US  Fox News   http://example6.com   \n",
       "6   45678901      5  2022-01-29      US       CNN   http://example7.com   \n",
       "7   10987654      5  2022-05-12      US  Fox News   http://example8.com   \n",
       "8   56789012      5  2022-09-25      US       CNN   http://example9.com   \n",
       "9   21098765      5  2022-12-14      US  Fox News  http://example10.com   \n",
       "10  67890123      5  2023-01-01      US       CNN  http://example11.com   \n",
       "11  32109876      5  2023-04-19      US  Fox News  http://example12.com   \n",
       "12  78901234      5  2023-07-05      US       CNN  http://example13.com   \n",
       "13  43210987      5  2023-10-08      US  Fox News  http://example14.com   \n",
       "14  89012345      5  2023-12-13      US       CNN  http://example15.com   \n",
       "15  54321098      5  2024-02-21      US  Fox News  http://example16.com   \n",
       "16  90123456      5  2024-05-07      US       CNN  http://example17.com   \n",
       "17  65432109      5  2024-08-25      US  Fox News  http://example18.com   \n",
       "18  11223344      5  2024-10-14      US       CNN  http://example19.com   \n",
       "19  44332211      5  2024-11-05      US  Fox News  http://example20.com   \n",
       "\n",
       "                   title                                               body  \n",
       "0    Title for article 1  In today窶冱 society, the importance of bodily a...  \n",
       "1    Title for article 2                               Abortion is not okay  \n",
       "2    Title for article 3                                   Abortion is good  \n",
       "3    Title for article 4                                    Abortion is bad  \n",
       "4    Title for article 5                     Abortion is something I am for  \n",
       "5    Title for article 6                Abortion is something I am against.  \n",
       "6    Title for article 7                                   Abortion is cool  \n",
       "7    Title for article 8                                 Abortion is uncool  \n",
       "8    Title for article 9                        Abortion is definitely nice  \n",
       "9   Title for article 10                   Abortion is definitely not nice.  \n",
       "10  Title for article 11                               Abortion is the best  \n",
       "11  Title for article 12                              Abortion is the worst  \n",
       "12  Title for article 13                                    I love Abortion  \n",
       "13  Title for article 14                                    I hate Abortion  \n",
       "14  Title for article 15                        Abortion is best thing ever  \n",
       "15  Title for article 16         Abortion is murder against innocent babies  \n",
       "16  Title for article 17       Abortion is a cornerstone of women's rights.  \n",
       "17  Title for article 18            Abortion should be banned in all states  \n",
       "18  Title for article 19  In today窶冱 society, the importance of bodily a...  \n",
       "19  Title for article 20  Abortion is unjust and takes away innocent lives.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the data for each column\n",
    "data = {\n",
    "    \"textID\": [12345678, 87654321, 23456789, 98765432, 34567890, \n",
    "               76543210, 45678901, 10987654, 56789012, 21098765,\n",
    "               67890123, 32109876, 78901234, 43210987, 89012345,\n",
    "               54321098, 90123456, 65432109, 11223344, 44332211],\n",
    "    \"words\": [5] * 20,\n",
    "    \"date\": [\n",
    "        \"2020-11-03\", \"2020-12-15\", \"2021-03-22\", \"2021-05-10\", \n",
    "        \"2021-10-05\", \"2021-12-18\", \"2022-01-29\", \"2022-05-12\",\n",
    "        \"2022-09-25\", \"2022-12-14\", \"2023-01-01\", \"2023-04-19\",\n",
    "        \"2023-07-05\", \"2023-10-08\", \"2023-12-13\", \"2024-02-21\", \n",
    "        \"2024-05-07\", \"2024-08-25\", \"2024-10-14\", \"2024-11-05\"\n",
    "    ],\n",
    "    \"country\": [\"US\"] * 20,\n",
    "    \"source\": [\"CNN\", \"Fox News\"] * 10,\n",
    "    \"url\": [f\"http://example{num}.com\" for num in range(1, 21)],\n",
    "    \"title\": [f\"Title for article {i}\" for i in range(1, 21)],\n",
    "    \"body\": [\n",
    "        \"In today窶冱 society, the importance of bodily autonomy cannot be overstated. Women around the world continue to fight for their right to make decisions about their own bodies, and one of the most crucial aspects of this fight is access to safe and legal abortion. Abortion is not just a healthcare issue; it is a fundamental right that empowers women, ensuring they have control over their futures. For decades, studies have shown that when women have access to safe abortion services, societies as a whole benefit. Countries with liberal abortion laws tend to have lower maternal mortality rates, improved public health outcomes, and greater gender equality. Women who can control their reproductive health are more likely to pursue education and career opportunities, contributing to the economic and social well-being of their communities. Opponents of abortion often frame it as a moral or religious issue, but this perspective fails to consider the complex realities many women face. For some, pregnancy may pose serious health risks. Others may be in abusive relationships or lack the financial stability to support a child. Denying these women the right to a safe abortion forces them into situations that can lead to physical, emotional, and economic harm. Moreover, restricting abortion does not stop it from happening窶琶t only makes it more dangerous. According to the World Health Organization, unsafe abortions are a leading cause of maternal death worldwide. In regions where abortion is heavily restricted or banned, women are often forced to seek unsafe, unregulated procedures, putting their lives at risk. In contrast, countries that provide accessible and affordable abortion services have significantly lower rates of complications and fatalities. It窶冱 also important to recognize that abortion is a deeply personal decision. No one is more qualified than the individual woman to determine what is best for her life and her circumstances. Each pregnancy is unique, and so are the challenges and decisions that come with it. For some women, continuing a pregnancy might be the right choice; for others, it might not be. The role of governments and healthcare systems should be to support women, not to impose restrictions that strip them of their agency. Abortion access is also a matter of social justice. Marginalized communities, including low-income women and women of color, are disproportionately affected by restrictive abortion laws. These groups often face systemic barriers to healthcare and are less likely to have access to contraception or comprehensive sex education. When abortion is restricted, they are the ones who suffer the most, perpetuating cycles of poverty and inequality. In conclusion, safe and legal abortion is a cornerstone of women窶冱 rights and healthcare. It empowers women to make choices about their own bodies, protects their health, and promotes equality. The debate around abortion should not be about controlling women窶冱 bodies but about supporting them in making informed decisions that are right for their lives. As societies progress, it is essential to uphold and expand access to safe abortion services, ensuring that every woman has the freedom to choose her own path\",\n",
    "        \"Abortion is not okay\",\n",
    "        \"Abortion is good\",\n",
    "        \"Abortion is bad\",\n",
    "        \"Abortion is something I am for\",\n",
    "        \"Abortion is something I am against.\",\n",
    "        \"Abortion is cool\",\n",
    "        \"Abortion is uncool\",\n",
    "        \"Abortion is definitely nice\",\n",
    "        \"Abortion is definitely not nice.\",\n",
    "        \"Abortion is the best\",\n",
    "        \"Abortion is the worst\",\n",
    "        \"I love Abortion\",\n",
    "        \"I hate Abortion\",\n",
    "        \"Abortion is best thing ever\",\n",
    "        \"Abortion is murder against innocent babies\",\n",
    "        \"Abortion is a cornerstone of women's rights.\",\n",
    "        \"Abortion should be banned in all states\",\n",
    "        \"In today窶冱 society, the importance of bodily autonomy cannot be overstated. Women around the world continue to fight for their right to make decisions about their own bodies, and one of the most crucial aspects of this fight is access to safe and legal abortion. Abortion is not just a healthcare issue; it is a fundamental right that empowers women, ensuring they have control over their futures. For decades, studies have shown that when women have access to safe abortion services, societies as a whole benefit. Countries with liberal abortion laws tend to have lower maternal mortality rates, improved public health outcomes, and greater gender equality. Women who can control their reproductive health are more likely to pursue education and career opportunities, contributing to the economic and social well-being of their communities. When women have the freedom to decide if and when to have children, they can fully participate in society, leading to stronger families, healthier communities, and more robust economies. The ability to access safe and legal abortion also reduces the burden on healthcare systems by preventing complications from unsafe procedures. In contrast, restrictive abortion laws force women into desperate situations, where they might seek dangerous and unregulated services. According to the World Health Organization, unsafe abortions are a leading cause of maternal death worldwide, with millions of women suffering from severe complications every year. The harm caused by these restrictions disproportionately affects marginalized communities, including low-income women and women of color, who often lack access to quality healthcare and family planning resources. Denying these women the right to a safe abortion perpetuates cycles of poverty and inequality, creating barriers that are difficult to overcome. Abortion opponents often argue that life begins at conception and that abortion is equivalent to taking a human life. While these beliefs are deeply held by some, they should not dictate public policy. The reality is that abortion is a deeply personal decision, one that is influenced by a wide range of individual circumstances, including health risks, financial stability, and personal goals. No two pregnancies are the same, and no one is better equipped than the woman herself to make decisions about her own body and her future. Governments should focus on creating policies that support women rather than imposing restrictions that strip them of their agency. When women are denied access to abortion, the consequences can be devastating. Unwanted pregnancies often result in significant financial strain, limiting opportunities for education and employment. Women may be forced to remain in abusive relationships or face social stigma and isolation. In some cases, carrying a pregnancy to term can pose serious health risks, endangering both the mother and the child. These are not theoretical concerns; they are real and affect millions of women around the world every year. Moreover, research has consistently shown that countries with more progressive abortion laws tend to have fewer abortions overall. This is because these countries prioritize comprehensive sex education and access to contraception, empowering individuals to make informed decisions about their reproductive health. By providing the tools and knowledge necessary to prevent unintended pregnancies, these countries create an environment where fewer women need to seek abortion services in the first place. In contrast, restrictive laws often lead to higher abortion rates, as they fail to address the root causes of unintended pregnancies. It is also important to recognize that abortion is a matter of healthcare. Just as individuals have the right to make decisions about other medical procedures, they should have the right to make decisions about abortion without interference from politicians or religious groups. The role of healthcare providers is to offer accurate information, compassionate care, and nonjudgmental support, ensuring that every patient can make the best decision for her unique situation. Abortion is not a one-size-fits-all issue, and it is essential that policies reflect the diverse needs and experiences of women. In addition to being a healthcare issue, abortion is fundamentally about human rights. Access to safe and legal abortion is recognized as a human right by organizations such as the United Nations and the World Health Organization. It is essential to ensure that all individuals, regardless of their socioeconomic status or geographic location, have the ability to exercise this right. When governments restrict access to abortion, they are not only violating human rights but also undermining public health and social progress. The stigma surrounding abortion must also be addressed. For too long, women who seek abortions have been shamed and silenced, forced to carry the weight of societal judgment. This stigma not only affects those who have had abortions but also creates a climate of fear and misinformation, discouraging individuals from seeking the care they need. By normalizing conversations about abortion and highlighting the experiences of women who have made this choice, we can foster a more supportive and understanding society. In conclusion, safe and legal abortion is a cornerstone of women窶冱 rights and healthcare. It empowers women to make choices about their own bodies, protects their health, and promotes equality. The debate around abortion should not be about controlling women窶冱 bodies but about supporting them in making informed decisions that are right for their lives. As societies progress, it is essential to uphold and expand access to safe abortion services, ensuring that every woman has the freedom to choose her own path. The future of reproductive rights depends on our collective commitment to justice, equality, and compassion. Now, more than ever, we must stand together to protect and defend the right to choose, recognizing that when women thrive, we all thrive\",\n",
    "        \"Abortion is unjust and takes away innocent lives.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000a5fa9-b36d-4c96-b12a-cf66f69c6d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>semantic_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.238415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.117612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.085108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.340992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.286112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  semantic_polarity\n",
       "0  2020           0.238415\n",
       "1  2021           0.117612\n",
       "2  2022           0.085108\n",
       "3  2023           0.340992\n",
       "4  2024           0.286112"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NO sliding window appraoch\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Function to get contextual embedding for a specific keyword in an article\n",
    "def get_contextual_embedding(text, keyword):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "    keyword_index = (tokens.input_ids == keyword_id).nonzero(as_tuple=True)[1]\n",
    "    \n",
    "    if len(keyword_index) > 0:\n",
    "        return output.last_hidden_state[0, keyword_index, :].mean(dim=0).numpy()\n",
    "    else:\n",
    "        return None  # Return None if keyword isn't found\n",
    "\n",
    "# Function to compute semantic polarity\n",
    "def compute_semantic_polarity(cnn_embeddings, fox_embeddings):\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    for cnn_emb in cnn_embeddings:\n",
    "        for fox_emb in fox_embeddings:\n",
    "            distance = 1 - cosine_similarity(cnn_emb.reshape(1, -1), fox_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    return total_distance / count if count > 0 else 0\n",
    "\n",
    "# Group articles by year\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "results = []\n",
    "\n",
    "for year, group in df.groupby('year'):\n",
    "    cnn_group = group[group['source'] == 'CNN']\n",
    "    fox_group = group[group['source'] == 'Fox News']\n",
    "    \n",
    "    # Extract embeddings for each article mentioning 'abortion'\n",
    "    cnn_embeddings = [get_contextual_embedding(body, 'abortion') for body in cnn_group['body'] if get_contextual_embedding(body, 'abortion') is not None]\n",
    "    fox_embeddings = [get_contextual_embedding(body, 'abortion') for body in fox_group['body'] if get_contextual_embedding(body, 'abortion') is not None]\n",
    "    \n",
    "    # Calculate semantic polarity for this year\n",
    "    sp_score = compute_semantic_polarity(cnn_embeddings, fox_embeddings)\n",
    "    results.append({'year': year, 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Semantic Polarity Over Time\", dataframe=sp_df)\n",
    "sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b42c116-6628-437a-8428-0dcb04d2e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  semantic_polarity\n",
      "0  2020           0.238415\n",
      "1  2021           0.117612\n",
      "2  2022           0.085108\n",
      "3  2023           0.340992\n",
      "4  2024           0.286112\n"
     ]
    }
   ],
   "source": [
    "# Sliding window approach沐･\n",
    "\n",
    "# Load BERT model and tokenizer (BERT-large is used for more capacity compared to BERT-base)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Function to extract contextual embedding for a specific keyword using a sliding window approach\n",
    "def get_contextual_embedding_sliding_window(text, keyword, max_len=512, stride=256):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for a specific keyword using BERT's sliding window approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The full article or text to process.\n",
    "    - keyword (str): The word to extract embeddings for.\n",
    "    - max_len (int): Maximum length of tokens per chunk (default is 512 for BERT).\n",
    "    - stride (int): Number of overlapping tokens between chunks (default is 256).\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: The averaged contextual embedding for the keyword across all chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the text into overlapping chunks\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',  # Return tensors in PyTorch format\n",
    "        padding=True,  # Pad tokens to max_len\n",
    "        truncation=True,  # Truncate tokens exceeding max_len\n",
    "        max_length=max_len,  # Limit chunk size to BERT's maximum of 512 tokens\n",
    "        stride=stride,  # Overlap between chunks\n",
    "        return_overflowing_tokens=True  # Return additional chunks as needed\n",
    "    )\n",
    "    \n",
    "    embeddings = []  # To store embeddings for the keyword across chunks\n",
    "    \n",
    "    # Loop through each chunk of tokens\n",
    "    for i in range(tokens['input_ids'].size(0)):  \n",
    "        # Select only necessary keys for BERT model input\n",
    "        chunk_tokens = {key: val[i].unsqueeze(0) for key, val in tokens.items() if key in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            output = model(**chunk_tokens)  # Pass chunk through BERT model\n",
    "        \n",
    "        # Get the token ID for the keyword\n",
    "        keyword_id = tokenizer.encode(keyword, add_special_tokens=False)[0]\n",
    "        \n",
    "        # Find the index of the keyword in the chunk\n",
    "        keyword_index = (chunk_tokens['input_ids'] == keyword_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(keyword_index[1]) > 0:  # If the keyword is found in the chunk\n",
    "            keyword_embedding = output.last_hidden_state[0, keyword_index[1], :].mean(dim=0).numpy()\n",
    "            embeddings.append(keyword_embedding)  # Collect the embedding\n",
    "    \n",
    "    # Return the averaged embedding across all chunks or None if the keyword is not found\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to compute semantic polarity between two sets of embeddings\n",
    "def compute_semantic_polarity(cnn_embeddings, fox_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the semantic polarity score between CNN and Fox News embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    - cnn_embeddings (list): List of embeddings for CNN articles.\n",
    "    - fox_embeddings (list): List of embeddings for Fox News articles.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average cosine distance (semantic polarity) between the two sets of embeddings.\n",
    "    \"\"\"\n",
    "    total_distance = 0  # Sum of cosine distances\n",
    "    count = 0  # Count of embedding pairs\n",
    "    \n",
    "    for cnn_emb in cnn_embeddings:\n",
    "        for fox_emb in fox_embeddings:\n",
    "            # Compute cosine similarity, then convert to distance (1 - similarity)\n",
    "            distance = 1 - cosine_similarity(cnn_emb.reshape(1, -1), fox_emb.reshape(1, -1))[0][0]\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    \n",
    "    return total_distance / count if count > 0 else 0  # Return average distance\n",
    "\n",
    "# Analyze semantic polarity over time\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year  # Extract year from the date column\n",
    "results = []  # Store results for each year\n",
    "\n",
    "for year, group in df.groupby('year'):  # Group articles by year\n",
    "    cnn_group = group[group['source'] == 'CNN']  # Filter CNN articles for the year\n",
    "    fox_group = group[group['source'] == 'Fox News']  # Filter Fox News articles for the year\n",
    "    \n",
    "    # Extract embeddings for 'abortion' from each article using sliding window\n",
    "    cnn_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') for body in cnn_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    fox_embeddings = [get_contextual_embedding_sliding_window(body, 'abortion') for body in fox_group['body'] if get_contextual_embedding_sliding_window(body, 'abortion') is not None]\n",
    "    \n",
    "    # Compute semantic polarity score for the year\n",
    "    sp_score = compute_semantic_polarity(cnn_embeddings, fox_embeddings)\n",
    "    results.append({'year': year, 'semantic_polarity': sp_score})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "sp_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(sp_df)  # Shows semantic polarity scores for each year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "385b92e1-34b4-48c3-869e-e7d5914c7dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set the plot size\n",
    "plt.plot(sp_df['year'], sp_df['semantic_polarity'], marker='o', linestyle='-', linewidth=2)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Semantic Polarity Over Time (CNN vs. Fox News on \"Abortion\")', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Semantic Polarity (SP)', fontsize=12)\n",
    "\n",
    "# Add grid and customize the x-ticks to show all years\n",
    "plt.grid(True)\n",
    "plt.xticks(sp_df['year'])\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd692a1-c03d-45ca-85e0-60718f800c2a",
   "metadata": {},
   "source": [
    "### I don't know if I like python's visualizations. So I'll save the semantic polarity scores as csv and visualize in r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb9d9c82-7216-47d6-990b-50421d9afee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the CSV沐･\n",
    "output_file_path = \"/work/Bachelor/results_for_plots/sp_df.csv\"\n",
    "\n",
    "# Save the filtered DataFrame as a CSV file\n",
    "sp_df.to_csv(output_file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e6515-635c-4554-a295-648d351df5c0",
   "metadata": {},
   "source": [
    "# Thoughts:\n",
    "\n",
    "## - Very small polarization scores... - I honestly don't know if it's a problem until I see the data. But bert-large-uncased produces higher polarity scores than bert-base-uncased汨ｩ窶昨沚ｳ\n",
    "## - is BERT good (bert-base-uncased)? Or would a SentenceTransformer be better?? - Daniel days it has to be a model that can do token level. So I guess BERT is fine. bert-large-uncased is better than bert-base-uncased.汨ｹ\n",
    "## - test that it actually takes all the text!! If in doubt, ask Daniel. Sentence-Transformer package might be the fix汨ｩ窶昨沚ｳ\n",
    "## - Which keywords should I check for? All from the query list, or just abortion, or just the ones that are present in both news outlets (find out from topic modeling on the ind汨ｩ窶昨沚ｳividual news sources) --> when you've figured it out, potentially modify code to be able to take more keywords, or duplicate code and use for the different keywords\n",
    "## - is subword tokenization et issue i det her tilfﾃｦlde i.e. kommer den til at cutte relevante tokens vﾃｦk ved at gﾃｸre det??汨ｩ窶昨沚ｳ\n",
    "## - WAIT is it unsmart to get only one unique embedding for the keyword and not keep the unique embeddings? read comment on drev for detailed thoughts + chat might have a code modification if you want to keep all unique embeddings汨ｩ窶昨沚ｳ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4449c31-e814-46d6-8712-dcc827e78b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
