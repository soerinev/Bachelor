{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd183ee-fe9a-4285-be88-9e3daa58b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install os\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "804de36d-a797-4b0c-8109-310a98d68aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see directory\n",
    "os. getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03939cd-8a7d-4828-ae6f-0442feb38c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split database.txt into smaller txt files\n",
    "def split_file_by_lines(file_path, lines_per_file):\n",
    "    \"\"\"Splits a large text file into smaller files with a specific number of lines each.\"\"\"\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        file_count = 1\n",
    "        current_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            current_lines.append(line)\n",
    "            \n",
    "            # When we've collected enough lines, write them to a new file\n",
    "            if len(current_lines) == lines_per_file:\n",
    "                with open(f'output_{file_count}.txt', 'w') as new_file:\n",
    "                    new_file.writelines(current_lines)\n",
    "                file_count += 1\n",
    "                current_lines = []  # Reset for the next file\n",
    "        \n",
    "        # Write any remaining lines to a final file\n",
    "        if current_lines:\n",
    "            with open(f'output_{file_count}.txt', 'w') as new_file:\n",
    "                new_file.writelines(current_lines)\n",
    "\n",
    "# Example usage\n",
    "split_file_by_lines('/work/Bachelor/sample_data/split/database.txt', 500000)  # Split the file by 100,000 lines per file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64f479a7-8acf-4e53-ae1d-58fb1332643d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1045112706.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    with open(f'/work/Bachelor/sample_data/split/output_{file_count}.txt', 'w') as new_file:\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#with open(f'/work/Bachelor/sample_data/split/output_{file_count}.txt', 'w') as new_file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88db6d9-d646-4ac3-89b9-62304a50fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line 10000\n",
      "Processing line 20000\n",
      "Processing line 30000\n",
      "Processing line 40000\n",
      "Processing line 50000\n",
      "Processing line 60000\n",
      "Processing line 70000\n",
      "Processing line 80000\n",
      "Processing line 90000\n",
      "Processing line 100000\n",
      "Processing line 110000\n",
      "Processing line 120000\n",
      "Processing line 130000\n",
      "Processing line 140000\n",
      "Processing line 150000\n",
      "Processing line 160000\n",
      "Processing line 170000\n",
      "Processing line 180000\n",
      "Processing line 190000\n",
      "Processing line 200000\n",
      "Processing line 210000\n",
      "Processing line 220000\n",
      "Processing line 230000\n",
      "Processing line 240000\n",
      "Processing line 250000\n",
      "Processing line 260000\n",
      "Processing line 270000\n",
      "Processing line 280000\n",
      "Processing line 290000\n",
      "Processing line 300000\n",
      "Processing line 310000\n",
      "Processing line 320000\n",
      "Processing line 330000\n",
      "Processing line 340000\n",
      "Processing line 350000\n",
      "Processing line 360000\n",
      "Processing line 370000\n",
      "Processing line 380000\n",
      "Processing line 390000\n",
      "Processing line 400000\n",
      "Processing line 410000\n",
      "Processing line 420000\n",
      "Processing line 430000\n",
      "Processing line 440000\n",
      "Processing line 450000\n",
      "Processing line 460000\n",
      "Processing line 470000\n",
      "Processing line 480000\n",
      "Processing line 490000\n",
      "Processing line 500000\n",
      "Writing file /work/Bachelor/sample_data/split/database_1.txt\n",
      "Processing line 10000\n",
      "Processing line 20000\n",
      "Processing line 30000\n",
      "Processing line 40000\n",
      "Processing line 50000\n",
      "Processing line 60000\n",
      "Processing line 70000\n",
      "Processing line 80000\n",
      "Processing line 90000\n",
      "Processing line 100000\n",
      "Processing line 110000\n",
      "Processing line 120000\n",
      "Processing line 130000\n",
      "Processing line 140000\n",
      "Processing line 150000\n",
      "Processing line 160000\n",
      "Processing line 170000\n",
      "Processing line 180000\n",
      "Processing line 190000\n",
      "Processing line 200000\n",
      "Processing line 210000\n",
      "Processing line 220000\n",
      "Processing line 230000\n",
      "Processing line 240000\n",
      "Processing line 250000\n",
      "Processing line 260000\n",
      "Processing line 270000\n",
      "Processing line 280000\n",
      "Processing line 290000\n",
      "Processing line 300000\n",
      "Processing line 310000\n",
      "Processing line 320000\n",
      "Processing line 330000\n",
      "Processing line 340000\n",
      "Processing line 350000\n",
      "Processing line 360000\n",
      "Processing line 370000\n",
      "Processing line 380000\n",
      "Processing line 390000\n",
      "Processing line 400000\n",
      "Processing line 410000\n",
      "Processing line 420000\n",
      "Processing line 430000\n",
      "Processing line 440000\n",
      "Processing line 450000\n",
      "Processing line 460000\n",
      "Processing line 470000\n",
      "Processing line 480000\n",
      "Processing line 490000\n",
      "Processing line 500000\n",
      "Writing file /work/Bachelor/sample_data/split/database_2.txt\n",
      "Processing line 10000\n",
      "Processing line 20000\n",
      "Processing line 30000\n",
      "Processing line 40000\n",
      "Processing line 50000\n",
      "Processing line 60000\n",
      "Processing line 70000\n",
      "Processing line 80000\n",
      "Processing line 90000\n",
      "Processing line 100000\n",
      "Processing line 110000\n",
      "Processing line 120000\n",
      "Processing line 130000\n",
      "Processing line 140000\n",
      "Processing line 150000\n",
      "Processing line 160000\n",
      "Processing line 170000\n",
      "Processing line 180000\n",
      "Processing line 190000\n",
      "Processing line 200000\n",
      "Processing line 210000\n",
      "Processing line 220000\n",
      "Processing line 230000\n",
      "Processing line 240000\n",
      "Processing line 250000\n",
      "Processing line 260000\n",
      "Processing line 270000\n",
      "Processing line 280000\n",
      "Processing line 290000\n",
      "Processing line 300000\n",
      "Processing line 310000\n",
      "Processing line 320000\n",
      "Processing line 330000\n",
      "Processing line 340000\n",
      "Processing line 350000\n",
      "Processing line 360000\n",
      "Processing line 370000\n",
      "Processing line 380000\n",
      "Processing line 390000\n",
      "Processing line 400000\n",
      "Processing line 410000\n",
      "Processing line 420000\n",
      "Processing line 430000\n",
      "Processing line 440000\n",
      "Processing line 450000\n",
      "Processing line 460000\n",
      "Processing line 470000\n",
      "Processing line 480000\n",
      "Processing line 490000\n",
      "Processing line 500000\n",
      "Writing file /work/Bachelor/sample_data/split/database_3.txt\n",
      "Processing line 10000\n",
      "Processing line 20000\n",
      "Processing line 30000\n",
      "Processing line 40000\n",
      "Processing line 50000\n",
      "Processing line 60000\n",
      "Processing line 70000\n",
      "Processing line 80000\n",
      "Processing line 90000\n",
      "Processing line 100000\n",
      "Processing line 110000\n",
      "Processing line 120000\n",
      "Processing line 130000\n",
      "Processing line 140000\n",
      "Processing line 150000\n",
      "Processing line 160000\n",
      "Processing line 170000\n",
      "Processing line 180000\n",
      "Processing line 190000\n",
      "Processing line 200000\n",
      "Processing line 210000\n",
      "Processing line 220000\n",
      "Processing line 230000\n",
      "Processing line 240000\n",
      "Processing line 250000\n",
      "Processing line 260000\n",
      "Processing line 270000\n",
      "Processing line 280000\n",
      "Processing line 290000\n",
      "Processing line 300000\n",
      "Processing line 310000\n",
      "Processing line 320000\n",
      "Processing line 330000\n",
      "Processing line 340000\n",
      "Processing line 350000\n",
      "Processing line 360000\n",
      "Processing line 370000\n",
      "Processing line 380000\n",
      "Processing line 390000\n",
      "Processing line 400000\n",
      "Processing line 410000\n",
      "Processing line 420000\n",
      "Processing line 430000\n",
      "Processing line 440000\n",
      "Processing line 450000\n",
      "Writing final file /work/Bachelor/sample_data/split/database_4.txt\n"
     ]
    }
   ],
   "source": [
    "# make function that can split txt files into smaller txt files\n",
    "import os\n",
    "\n",
    "def split_file_by_lines(file_path, lines_per_file):\n",
    "    \"\"\"Splits a large text file into smaller files with a specific number of lines each.\"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_folder = os.path.dirname(file_path)  # Get the directory of the input file\n",
    "    base_filename = os.path.splitext(os.path.basename(file_path))[0]  # Get the base name of the input file (without extension)\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_count = 1\n",
    "        current_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            current_lines.append(line)\n",
    "            \n",
    "            # Print progress for debugging\n",
    "            if len(current_lines) % 10000 == 0:\n",
    "                print(f\"Processing line {len(current_lines)}\")\n",
    "            \n",
    "            # When we've collected enough lines, write them to a new file\n",
    "            if len(current_lines) == lines_per_file:\n",
    "                output_file_path = os.path.join(output_folder, f'{base_filename}_{file_count}.txt')\n",
    "                print(f\"Writing file {output_file_path}\")\n",
    "                with open(output_file_path, 'w') as new_file:\n",
    "                    new_file.writelines(current_lines)\n",
    "                file_count += 1\n",
    "                current_lines = []  # Reset for the next file\n",
    "        \n",
    "        # Write any remaining lines to a final file\n",
    "        if current_lines:\n",
    "            output_file_path = os.path.join(output_folder, f'{base_filename}_{file_count}.txt')\n",
    "            print(f\"Writing final file {output_file_path}\")\n",
    "            with open(output_file_path, 'w') as new_file:\n",
    "                new_file.writelines(current_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97823269-0efb-447a-a958-078dfcd275ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function to split database.txt into smaller txt files\n",
    "split_file_by_lines('/work/Bachelor/sample_data/split/database.txt', 500000)  # Split the file by 500,000 lines per file + the remaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f149c8cc-1327-47f3-ab57-50b20c080009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line 10000\n",
      "Processing line 20000\n",
      "Processing line 30000\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x82 in position 1300: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use function to split wordLem_poS.txt into smaller txt files\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msplit_file_by_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/work/Bachelor/sample_data/split/wordLem_poS.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Split the file by 500,000 lines per file + the remaining\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36msplit_file_by_lines\u001b[0;34m(file_path, lines_per_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m file_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m current_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     15\u001b[0m     current_lines\u001b[38;5;241m.\u001b[39mappend(line)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Print progress for debugging\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x82 in position 1300: invalid start byte"
     ]
    }
   ],
   "source": [
    "# use function to split wordLem_poS.txt into smaller txt files\n",
    "split_file_by_lines('/work/Bachelor/sample_data/split/wordLem_poS.txt', 500000)  # Split the file by 500,000 lines per file + the remaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b27ea03f-aa57-479d-b147-320807c1be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/Bachelor/sample_data/split/database_1.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b24266-d6f1-4da0-9198-c225427be210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
