{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d3da2b-5f42-4422-9cc7-45c4022b1fbc",
   "metadata": {},
   "source": [
    "# No sentiment analysis no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90b789a-fed0-4f5f-b3a0-98c19ea8e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for as\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.1.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, srsly, smart-open, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.0\n",
      "    Uninstalling numpy-2.1.0:\n",
      "      Successfully uninstalled numpy-2.1.0\n",
      "Successfully installed blis-1.0.1 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-2.0.2 preshed-3.0.9 rich-13.9.3 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.24.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (24.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting plotly.express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from plotly.express) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from plotly.express) (5.24.0)\n",
      "Collecting statsmodels>=0.9.0 (from plotly.express)\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting scipy>=0.18 (from plotly.express)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting patsy>=0.5 (from plotly.express)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.10/site-packages (from plotly.express) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.20.0->plotly.express) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.20.0->plotly.express) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.20.0->plotly.express) (2024.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5->plotly.express) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=4.1.0->plotly.express) (9.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly>=4.1.0->plotly.express) (24.0)\n",
      "Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, patsy, statsmodels, plotly.express\n",
      "Successfully installed patsy-0.5.6 plotly.express-0.4.1 scipy-1.14.1 statsmodels-0.14.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting NewsSentiment\n",
      "  Downloading NewsSentiment-1.2.28-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting boto3>=1.19.7 (from NewsSentiment)\n",
      "  Downloading boto3-1.35.52-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting gensim>=4.0.1 (from NewsSentiment)\n",
      "  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting imbalanced-learn>=0.8.1 (from NewsSentiment)\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jsonlines>=2.0.0 (from NewsSentiment)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: matplotlib>=3.4.3 in /opt/conda/lib/python3.10/site-packages (from NewsSentiment) (3.9.2)\n",
      "Collecting networkx>=2.6.3 (from NewsSentiment)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting openpyxl>=3.0.5 (from NewsSentiment)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pandas>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from NewsSentiment) (2.2.2)\n",
      "Collecting regex>=2021.10.23 (from NewsSentiment)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from NewsSentiment) (2.31.0)\n",
      "Collecting sacremoses>=0.0.46 (from NewsSentiment)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting scikit-learn>=1.0.1 (from NewsSentiment)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: spacy>=3.2 in /opt/conda/lib/python3.10/site-packages (from NewsSentiment) (3.8.2)\n",
      "Collecting tabulate>=0.8.9 (from NewsSentiment)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /opt/conda/lib/python3.10/site-packages (from NewsSentiment) (4.66.2)\n",
      "Collecting transformers<=4.24,>=4.17 (from NewsSentiment)\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting torch<2.1,>=1.12 (from NewsSentiment)\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.52 (from boto3>=1.19.7->NewsSentiment)\n",
      "  Downloading botocore-1.35.52-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.19.7->NewsSentiment)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.19.7->NewsSentiment)\n",
      "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim>=4.0.1->NewsSentiment) (7.0.5)\n",
      "Collecting joblib>=1.1.1 (from imbalanced-learn>=0.8.1->NewsSentiment)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from imbalanced-learn>=0.8.1->NewsSentiment)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines>=2.0.0->NewsSentiment) (24.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.4.3->NewsSentiment) (2.9.0)\n",
      "Collecting et-xmlfile (from openpyxl>=3.0.5->NewsSentiment)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.3->NewsSentiment) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.3->NewsSentiment) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->NewsSentiment) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->NewsSentiment) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->NewsSentiment) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->NewsSentiment) (2024.7.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses>=0.0.46->NewsSentiment) (8.1.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (69.5.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2->NewsSentiment) (3.4.1)\n",
      "Collecting filelock (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.12->NewsSentiment) (4.12.2)\n",
      "Collecting sympy (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.1,>=1.12->NewsSentiment) (0.43.0)\n",
      "Collecting cmake (from triton==2.0.0->torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading cmake-3.30.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting lit (from triton==2.0.0->torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0 (from transformers<=4.24,>=4.17->NewsSentiment)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.24,>=4.17->NewsSentiment) (6.0.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<=4.24,>=4.17->NewsSentiment)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.10.0->transformers<=4.24,>=4.17->NewsSentiment)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2->NewsSentiment) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->NewsSentiment) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->NewsSentiment) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.3->NewsSentiment) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim>=4.0.1->NewsSentiment) (1.16.0)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.2->NewsSentiment) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.2->NewsSentiment) (0.1.5)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy>=3.2->NewsSentiment)\n",
      "  Downloading thinc-8.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "  Downloading thinc-8.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "INFO: pip is still looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading scipy-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "  Downloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim>=4.0.1->NewsSentiment)\n",
      "  Downloading scipy-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "  Downloading scipy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading scipy-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.4.3->NewsSentiment)\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "INFO: pip is still looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading contourpy-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "  Downloading contourpy-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "  Downloading contourpy-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "  Downloading contourpy-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "  Downloading contourpy-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "  Downloading contourpy-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting transformers<=4.24,>=4.17 (from NewsSentiment)\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl.metadata (88 kB)\n",
      "  Downloading transformers-4.23.0-py3-none-any.whl.metadata (88 kB)\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl.metadata (84 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<=4.24,>=4.17->NewsSentiment)\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting transformers<=4.24,>=4.17 (from NewsSentiment)\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl.metadata (84 kB)\n",
      "  Downloading transformers-4.22.0-py3-none-any.whl.metadata (84 kB)\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl.metadata (81 kB)\n",
      "  Downloading transformers-4.21.2-py3-none-any.whl.metadata (81 kB)\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl.metadata (81 kB)\n",
      "  Downloading transformers-4.21.0-py3-none-any.whl.metadata (81 kB)\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n",
      "  Downloading transformers-4.20.0-py3-none-any.whl.metadata (77 kB)\n",
      "  Downloading transformers-4.19.4-py3-none-any.whl.metadata (73 kB)\n",
      "  Downloading transformers-4.19.3-py3-none-any.whl.metadata (73 kB)\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl.metadata (73 kB)\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl.metadata (73 kB)\n",
      "  Downloading transformers-4.19.0-py3-none-any.whl.metadata (73 kB)\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1 (from transformers<=4.24,>=4.17->NewsSentiment)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting spacy>=3.2 (from NewsSentiment)\n",
      "  Using cached spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "  Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy>=3.2->NewsSentiment)\n",
      "  Downloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy>=3.2->NewsSentiment)\n",
      "  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2->NewsSentiment) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2->NewsSentiment) (13.9.3)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2->NewsSentiment) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy>=3.2->NewsSentiment) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch<2.1,>=1.12->NewsSentiment)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2->NewsSentiment) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2->NewsSentiment) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2->NewsSentiment) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2->NewsSentiment) (0.1.2)\n",
      "Downloading NewsSentiment-1.2.28-py3-none-any.whl (919 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.0/919.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.35.52-py3-none-any.whl (139 kB)\n",
      "Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.35.52-py3-none-any.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
      "Downloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.30.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: tokenizers, mpmath, lit, threadpoolctl, tabulate, sympy, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, jsonlines, joblib, jmespath, fsspec, filelock, et-xmlfile, cmake, scipy, sacremoses, openpyxl, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, botocore, blis, transformers, scikit-learn, s3transfer, gensim, thinc, imbalanced-learn, boto3, spacy, triton, torch, NewsSentiment\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.0.1\n",
      "    Uninstalling blis-1.0.1:\n",
      "      Successfully uninstalled blis-1.0.1\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.2\n",
      "    Uninstalling thinc-8.3.2:\n",
      "      Successfully uninstalled thinc-8.3.2\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.2\n",
      "    Uninstalling spacy-3.8.2:\n",
      "      Successfully uninstalled spacy-3.8.2\n",
      "Successfully installed NewsSentiment-1.2.28 blis-0.7.11 boto3-1.35.52 botocore-1.35.52 cmake-3.30.5 et-xmlfile-2.0.0 filelock-3.16.1 fsspec-2024.10.0 gensim-4.3.3 huggingface-hub-0.26.2 imbalanced-learn-0.12.4 jmespath-1.0.1 joblib-1.4.2 jsonlines-4.0.0 lit-18.1.8 mpmath-1.3.0 networkx-3.4.2 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 openpyxl-3.1.5 regex-2024.9.11 s3transfer-0.10.3 sacremoses-0.1.1 scikit-learn-1.5.2 scipy-1.13.1 spacy-3.7.5 sympy-1.13.3 tabulate-0.9.0 thinc-8.2.5 threadpoolctl-3.5.0 tokenizers-0.13.3 torch-2.0.1 transformers-4.24.0 triton-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# import/load packages\n",
    "\n",
    "## install\n",
    "%pip install pandas as pd\n",
    "%pip install spacy\n",
    "%pip install plotly\n",
    "%pip install plotly.express\n",
    "%pip install NewsSentiment\n",
    "\n",
    "## import\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import plotly\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from NewsSentiment import TargetSentimentClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee70122-439f-4349-8878-a6038fac5964",
   "metadata": {},
   "source": [
    "## NewsSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74746b70-5f89-4401-b522-4bf1e7e1a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I like \", \"Peter\", \" but I don't like Robert.\" ),\n",
    "    (\"\", \"Abortion\", \" is health care.\"),\n",
    "    (\"I hate \", \"Peter\", \" but I love Robert even more.\"),\n",
    "    (\"I am neither angry nor sad about \", \"this.\", \"\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7bb32ce-ccb4-4fb0-b928-71dcbe5424ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 4/4 [00:00<00:00,  7.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0 {'class_id': 1, 'class_label': 'neutral', 'class_prob': 0.4806241989135742}\n",
      "Sentiment:  1 {'class_id': 2, 'class_label': 'positive', 'class_prob': 0.5593082308769226}\n",
      "Sentiment:  2 {'class_id': 0, 'class_label': 'negative', 'class_prob': 0.8596703410148621}\n",
      "Sentiment:  3 {'class_id': 1, 'class_label': 'neutral', 'class_prob': 0.4737212061882019}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "\n",
    "sentiments = tsc.infer(targets=data)\n",
    "\n",
    "for i, result in enumerate(sentiments):\n",
    "    print(\"Sentiment: \", i, result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df3b50d5-33d5-488d-ba3f-e127a8e8ccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  5.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_id': 2, 'class_label': 'positive', 'class_prob': 0.4334748387336731}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "sentiment = tsc.infer_from_text(\"I like \" ,\"Peter\", \" but I don't like Robert.\")\n",
    "print(sentiment[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513914f4-2252-4dd2-870a-9a5147bbfc43",
   "metadata": {},
   "source": [
    "### Segmentation with NER (named entity recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e5ab2c5-82e1-44df-a076-9d044672a5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651355c6ec2346119c5195980de2d043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcddd6c83dc04e09bbe405d59eb85323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f063e81289460eaf70492be868a41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa73f05dbee4dc8b23e9cbd79d5bc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c617f2d55c4c4093fa2f29ac63d5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: ['Supreme', 'Court', 'Roe', 'v', 'Wade', 'Do', '##bbs', 'Jackson', 'Women', '##s', 'Health', 'Organization', 'Mississippi', 'Republican', 'Mississippi', 'Supreme', 'Court', 'Constitution', 'Roe', 'Casey', 'Samuel', 'Ali', '##to', 'Ali', 'Roe', 'Wade']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b4f9ffd28f4f898f2956601ac4cdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514604c419c4447b887a47f93b3448d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d5affa931842458d54396c69a03938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d734c62b08d4f16b579787810de842c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503129b86eb244d09e95fba1bf7f6da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Downloading to /home/ucloud/.cache/torch/hub/pretrained_models/state_dicts/grutsc_v1-0-0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 584M/584M [00:10<00:00, 57.5MB/s] \n",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tneutral\t0.59\tSupreme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tneutral\t0.57\tCourt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-MISC\tnegative\t0.77\tRoe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-MISC\tnegative\t0.58\tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-MISC\tnegative\t0.84\tWade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER\tneutral\t0.49\tDo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-PER\tneutral\t0.59\tbbs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tneutral\t0.49\tJackson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tneutral\t0.45\tWomen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tnegative\t0.48\ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tnegative\t0.47\tHealth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tnegative\t0.48\tOrganization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-LOC\tnegative\t0.45\tMississippi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-MISC\tneutral\t0.55\tRepublican\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-LOC\tnegative\t0.45\tMississippi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tneutral\t0.59\tSupreme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tneutral\t0.57\tCourt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-MISC\tnegative\t0.43\tConstitution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER\tnegative\t0.77\tRoe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER\tnegative\t0.82\tCasey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER\tneutral\t0.64\tSamuel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-PER\tneutral\t0.49\tAli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  2.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-PER\tnegative\t0.48\tto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "TooLongTextException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooLongTextException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m m \u001b[38;5;241m=\u001b[39m TEXT[span[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]:span[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     17\u001b[0m r \u001b[38;5;241m=\u001b[39m TEXT[span[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]:]\n\u001b[0;32m---> 18\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mtsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspan[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msentiment[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msentiment[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/infer.py:86\u001b[0m, in \u001b[0;36mTargetSentimentClassifier.infer_from_text\u001b[0;34m(self, left, target, right)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, left, target, right):\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Calculates the targeted mention of the sentence defined by left + target + right.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    :param left: The text from the beginning of the sentence to the mention of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_right\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/infer.py:217\u001b[0m, in \u001b[0;36mTargetSentimentClassifier.infer\u001b[0;34m(self, text_left, target_mention, text_right, text, target_mention_from, target_mention_to, targets, batch_size, disable_tqdm)\u001b[0m\n\u001b[1;32m    205\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_start, batch_end \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_targets, batch_size),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable_tqdm,\n\u001b[1;32m    216\u001b[0m ):\n\u001b[0;32m--> 217\u001b[0m     out\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_targets_based \u001b[38;5;28;01melse\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/infer.py:279\u001b[0m, in \u001b[0;36mTargetSentimentClassifier.batch_infer\u001b[0;34m(self, targets)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# assert text_right.startswith(' ')\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     targets_prepared\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    272\u001b[0m         (\n\u001b[1;32m    273\u001b[0m             FXEasyTokenizer\u001b[38;5;241m.\u001b[39mprepare_left_segment(text_left),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[1;32m    277\u001b[0m     )\n\u001b[0;32m--> 279\u001b[0m indexed_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_create_model_input_seqs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets_prepared\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoreferential_targets_for_target_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstructor\u001b[38;5;241m.\u001b[39mselect_inputs(indexed_examples, is_single_item\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    286\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/dataset.py:967\u001b[0m, in \u001b[0;36mFXEasyTokenizer.batch_create_model_input_seqs\u001b[0;34m(self, targets, coreferential_targets_for_target_mask)\u001b[0m\n\u001b[1;32m    957\u001b[0m target_ids_with_special_tokens_per_target \u001b[38;5;241m=\u001b[39m tok_obj(\n\u001b[1;32m    958\u001b[0m     target_phrases,\n\u001b[1;32m    959\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest_first\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    963\u001b[0m )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# create target masks\u001b[39;00m\n\u001b[1;32m    966\u001b[0m target_mask_seq_for_text_with_special_tokens_per_target \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_create_target_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtok_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfor_text_with_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m )\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# create target mask for coreferential targets of given target\u001b[39;00m\n\u001b[1;32m    972\u001b[0m coref_target_masks_per_target \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_create_coreferential_target_masks(\n\u001b[1;32m    974\u001b[0m         tok_obj, coreferential_targets_for_target_mask\n\u001b[1;32m    975\u001b[0m     )\n\u001b[1;32m    976\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/dataset.py:474\u001b[0m, in \u001b[0;36mFXEasyTokenizer._batch_create_target_mask\u001b[0;34m(self, tokenizer, targets, for_text_with_special_tokens, is_raise_exception_if_target_after_max_seq_len)\u001b[0m\n\u001b[1;32m    468\u001b[0m     rights\u001b[38;5;241m.\u001b[39mappend(target[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    470\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_for_target_mask(\n\u001b[1;32m    471\u001b[0m     tokenizer, lefts, target_phrases, rights, full_texts\n\u001b[1;32m    472\u001b[0m )\n\u001b[0;32m--> 474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_target_mask_on_encoding(\n\u001b[1;32m    476\u001b[0m         tokenizer, \u001b[38;5;241m*\u001b[39mencoding, is_raise_exception_if_target_after_max_seq_len\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mencodings)\n\u001b[1;32m    479\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/dataset.py:475\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    468\u001b[0m     rights\u001b[38;5;241m.\u001b[39mappend(target[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    470\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_for_target_mask(\n\u001b[1;32m    471\u001b[0m     tokenizer, lefts, target_phrases, rights, full_texts\n\u001b[1;32m    472\u001b[0m )\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_target_mask_on_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_raise_exception_if_target_after_max_seq_len\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mencodings)\n\u001b[1;32m    479\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NewsSentiment/dataset.py:561\u001b[0m, in \u001b[0;36mFXEasyTokenizer._create_target_mask_on_encoding\u001b[0;34m(self, tokenizer, text_left_ids_with_special_tokens, target_phrase_ids_with_special_tokens, text_right_ids, text_ids_with_special_tokens, is_raise_exception_if_target_after_max_seq_len)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tp_seq_in_text_seq_pos_end \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_raise_exception_if_target_after_max_seq_len:\n\u001b[0;32m--> 561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TooLongTextException()\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;66;03m# e.g., if invoked for creating the target mapping\u001b[39;00m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoolong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTooLongTextException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "TEXT = \"The Supreme Court on Friday overturned Roe v Wade effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow limit or ban the practice altogether The ruling came in the courts opinion in Dobbs v Jackson Womens Health Organization which centered on a Mississippi law that banned abortion after 15 weeks of pregnancy The Republican led state of Mississippi asked the Supreme Court to strike down a lower court ruling that stopped the 15 week abortion ban from taking place We end this opinion where we began Abortion presents a profound moral question The Constitution does not prohibit the citizens of each State from regulating or prohibiting abortion Roe and Casey arrogated that authority We now overrule those decisions and return that authority to the people and their elected representatives Justice Samuel Alito wrote in the courts opinion Alitos opinion began with an exploration and criticism of Roe v Wade and its holding that while states have a legitimate interest in protecting potential life this interest was not strong enough to prohibit abortions before the time of fetal viability understood to be at about 23 weeks into pregnancy\"\n",
    "ner_spans = nlp(TEXT)\n",
    "ents = [span[\"word\"] for span in ner_spans]\n",
    "print(f\"Entities: {ents}\")\n",
    "tsc = TargetSentimentClassifier()\n",
    "for span in ner_spans:\n",
    "    l = TEXT[:span['start']]\n",
    "    m = TEXT[span['start']:span['end']]\n",
    "    r = TEXT[span['end']:]\n",
    "    sentiment = tsc.infer_from_text(l, m, r)\n",
    "    print(f\"{span['entity']}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daeac1c9-7a71-44b4-a00d-ae26a8e92d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: ['John', 'Robert']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  8.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER\tpositive\t0.47\tJohn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  9.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER\tnegative\t0.82\tRobert\n",
      "Modified Text with Quoted Parts:\n",
      "\"I like \", \"John\", \" but I hate \", \"Robert\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "# Load the tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize sentiment classifier\n",
    "tsc = TargetSentimentClassifier()\n",
    "\n",
    "def process_text_with_quotation_and_sentiment(text):\n",
    "    # Perform named entity recognition\n",
    "    ner_spans = nlp(text)\n",
    "    \n",
    "    # Extract entities and print the list\n",
    "    ents = [span[\"word\"] for span in ner_spans]\n",
    "    print(f\"Entities: {ents}\")\n",
    "\n",
    "    # Initialize positions and result list for quoted text parts\n",
    "    last_end = 0\n",
    "    quoted_text_parts = []\n",
    "\n",
    "    for span in ner_spans:\n",
    "        start, end = span['start'], span['end']\n",
    "\n",
    "        # Add the text before the entity in quotes\n",
    "        if last_end < start:\n",
    "            quoted_text_parts.append(f'\"{text[last_end:start]}\"')\n",
    "        \n",
    "        # Add the entity in quotes\n",
    "        quoted_text_parts.append(f'\"{text[start:end]}\"')\n",
    "        \n",
    "        # Extract left, middle, right for sentiment analysis\n",
    "        l = text[:start]\n",
    "        m = text[start:end]\n",
    "        r = text[end:]\n",
    "        \n",
    "        # Infer sentiment and print the results\n",
    "        sentiment = tsc.infer_from_text(l, m, r)\n",
    "        print(f\"{span['entity']}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{m}\")\n",
    "\n",
    "        # Update last end position to the end of this entity\n",
    "        last_end = end\n",
    "\n",
    "    # Add the final part of the text after the last entity\n",
    "    if last_end < len(text):\n",
    "        quoted_text_parts.append(f'\"{text[last_end:]}\"')\n",
    "    \n",
    "    # Join the quoted text parts and print\n",
    "    quoted_text = ', '.join(quoted_text_parts)\n",
    "    print(\"Modified Text with Quoted Parts:\")\n",
    "    print(quoted_text)\n",
    "\n",
    "    return quoted_text\n",
    "\n",
    "# Define a shorter text for testing\n",
    "TEXT = \"I like John but I hate Robert\"\n",
    "\n",
    "# Run the function on the test text\n",
    "modified_text = process_text_with_quotation_and_sentiment(TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdb1bfa4-4a13-4e3d-917c-c3be7f0549df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: ['Supreme', 'Court', 'Roe', 'v', 'Wade']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  7.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tneutral\t0.86\tSupreme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  7.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-ORG\tneutral\t0.79\tCourt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  7.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-MISC\tnegative\t0.77\tRoe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  7.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-MISC\tnegative\t0.50\tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  7.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-MISC\tnegative\t0.94\tWade\n",
      "Modified Text with Quoted Parts:\n",
      "\"The \", \"Supreme\", \" \", \"Court\", \" on Friday overturned \", \"Roe\", \" \", \"v\", \" \", \"Wade\", \" effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow limit or ban the practice altogether\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 5/5 [00:00<00:00,  7.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0 {'class_id': 1, 'class_label': 'neutral', 'class_prob': 0.864378035068512}\n",
      "Sentiment:  1 {'class_id': 1, 'class_label': 'neutral', 'class_prob': 0.79281085729599}\n",
      "Sentiment:  2 {'class_id': 0, 'class_label': 'negative', 'class_prob': 0.7694401144981384}\n",
      "Sentiment:  3 {'class_id': 0, 'class_label': 'negative', 'class_prob': 0.495899498462677}\n",
      "Sentiment:  4 {'class_id': 0, 'class_label': 'negative', 'class_prob': 0.9367616772651672}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "# Load the tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize sentiment classifier\n",
    "tsc = TargetSentimentClassifier()\n",
    "\n",
    "def process_text_and_prepare_data(text):\n",
    "    # Perform named entity recognition\n",
    "    ner_spans = nlp(text)\n",
    "    \n",
    "    # Extract entities and print the list\n",
    "    ents = [span[\"word\"] for span in ner_spans]\n",
    "    print(f\"Entities: {ents}\")\n",
    "\n",
    "    # Initialize positions, result list for quoted text parts, and structured data\n",
    "    last_end = 0\n",
    "    quoted_text_parts = []\n",
    "    data_for_analysis = []\n",
    "\n",
    "    for span in ner_spans:\n",
    "        start, end = span['start'], span['end']\n",
    "\n",
    "        # Segment the text before the entity\n",
    "        pre_entity_text = text[last_end:start]\n",
    "        \n",
    "        # Entity text\n",
    "        entity_text = text[start:end]\n",
    "        \n",
    "        # Text after the entity up to the next segment\n",
    "        post_entity_text = text[end:]\n",
    "        \n",
    "        # Add to quoted text parts for the final output format\n",
    "        quoted_text_parts.append(f'\"{pre_entity_text}\"')\n",
    "        quoted_text_parts.append(f'\"{entity_text}\"')\n",
    "\n",
    "        # Update data_for_analysis with the tuple\n",
    "        data_for_analysis.append((pre_entity_text, entity_text, post_entity_text))\n",
    "\n",
    "        # Print the entity sentiment information\n",
    "        sentiment = tsc.infer_from_text(pre_entity_text, entity_text, post_entity_text)\n",
    "        print(f\"{span['entity']}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{entity_text}\")\n",
    "\n",
    "        # Update last_end position to continue processing the rest of the text\n",
    "        last_end = end\n",
    "\n",
    "    # Final segment after the last entity (if any) and add closing quotes\n",
    "    if last_end < len(text):\n",
    "        quoted_text_parts.append(f'\"{text[last_end:]}\"')\n",
    "    \n",
    "    # Join the quoted text parts\n",
    "    quoted_text = ', '.join(quoted_text_parts)\n",
    "    print(\"Modified Text with Quoted Parts:\")\n",
    "    print(quoted_text)\n",
    "\n",
    "    return data_for_analysis  # Return structured data for analysis\n",
    "\n",
    "# Define a shorter text for testing\n",
    "TEXT = \"The Supreme Court on Friday overturned Roe v Wade effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow limit or ban the practice altogether\"\n",
    "\n",
    "# Run the function on the test text and save the structured data\n",
    "data_for_analysis = process_text_and_prepare_data(TEXT)\n",
    "\n",
    "# Run sentiment analysis on the prepared data\n",
    "sentiments = tsc.infer(targets=data_for_analysis)\n",
    "\n",
    "# Display the sentiment results\n",
    "for i, result in enumerate(sentiments):\n",
    "    print(\"Sentiment: \", i, result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73298883-e9ed-4dda-a000-1772a3514c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: []\n",
      "Modified Text with Quoted Parts:\n",
      "\"I am neither angry nor sad about this\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 0batch [00:00, ?batch/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "# Load the tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize sentiment classifier\n",
    "tsc = TargetSentimentClassifier()\n",
    "\n",
    "def process_text_with_manual_target(text, manual_target=\"Roe v Wade\"):\n",
    "    # Perform named entity recognition\n",
    "    ner_spans = nlp(text)\n",
    "    \n",
    "    # Extract entities and manually add the target if present\n",
    "    ents = [span[\"word\"] for span in ner_spans]\n",
    "    print(f\"Entities: {ents}\")\n",
    "    \n",
    "    # Initialize positions, result list for quoted text parts, and structured data\n",
    "    last_end = 0\n",
    "    quoted_text_parts = []\n",
    "    data_for_analysis = []\n",
    "\n",
    "    # Check for manually specified target\n",
    "    target_index = text.find(manual_target)\n",
    "    if target_index != -1:\n",
    "        # Get context around \"Roe v Wade\"\n",
    "        pre_target_text = text[:target_index]\n",
    "        post_target_text = text[target_index + len(manual_target):]\n",
    "\n",
    "        # Append to the data for analysis with manual target\n",
    "        data_for_analysis.append((pre_target_text, manual_target, post_target_text))\n",
    "\n",
    "        # Also add it to quoted text parts for consistent output\n",
    "        quoted_text_parts.append(f'\"{pre_target_text}\"')\n",
    "        quoted_text_parts.append(f'\"{manual_target}\"')\n",
    "        last_end = target_index + len(manual_target)\n",
    "\n",
    "        # Print sentiment analysis for manual target\n",
    "        sentiment = tsc.infer_from_text(pre_target_text, manual_target, post_target_text)\n",
    "        print(f\"{manual_target}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{manual_target}\")\n",
    "\n",
    "    # Process automated NER spans\n",
    "    for span in ner_spans:\n",
    "        start, end = span['start'], span['end']\n",
    "\n",
    "        # Skip the manual target if it overlaps with NER spans\n",
    "        if start < last_end:\n",
    "            continue\n",
    "\n",
    "        # Segment the text before the entity\n",
    "        pre_entity_text = text[last_end:start]\n",
    "        \n",
    "        # Entity text\n",
    "        entity_text = text[start:end]\n",
    "        \n",
    "        # Text after the entity up to the next segment\n",
    "        post_entity_text = text[end:]\n",
    "        \n",
    "        # Add to quoted text parts for the final output format\n",
    "        quoted_text_parts.append(f'\"{pre_entity_text}\"')\n",
    "        quoted_text_parts.append(f'\"{entity_text}\"')\n",
    "\n",
    "        # Update data_for_analysis with the tuple\n",
    "        data_for_analysis.append((pre_entity_text, entity_text, post_entity_text))\n",
    "\n",
    "        # Print the entity sentiment information\n",
    "        sentiment = tsc.infer_from_text(pre_entity_text, entity_text, post_entity_text)\n",
    "        print(f\"{span['entity']}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{entity_text}\")\n",
    "\n",
    "        # Update last_end position to continue processing the rest of the text\n",
    "        last_end = end\n",
    "\n",
    "    # Final segment after the last entity (if any) and add closing quotes\n",
    "    if last_end < len(text):\n",
    "        quoted_text_parts.append(f'\"{text[last_end:]}\"')\n",
    "    \n",
    "    # Join the quoted text parts\n",
    "    quoted_text = ', '.join(quoted_text_parts)\n",
    "    print(\"Modified Text with Quoted Parts:\")\n",
    "    print(quoted_text)\n",
    "\n",
    "    return data_for_analysis  # Return structured data for analysis\n",
    "\n",
    "# Define a sample text containing the manual target \"Roe v Wade\"\n",
    "TEXT = \"I am neither angry nor sad about this\"\n",
    "\n",
    "# Run the function on the sample text and save the structured data\n",
    "data_for_analysis = process_text_with_manual_target(TEXT)\n",
    "\n",
    "# Run sentiment analysis on the prepared data\n",
    "sentiments = tsc.infer(targets=data_for_analysis)\n",
    "\n",
    "# Display the sentiment results\n",
    "for i, result in enumerate(sentiments):\n",
    "    print(\"Sentiment: \", i, result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39a9f7bb-8376-474d-9ad4-6f5812dd0172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: ['Supreme', 'Court', 'Roe', 'V', '.', 'Wade']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  5.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roe V. Wade\tnegative\t0.94\tRoe V. Wade\n",
      "Modified Text with Quoted Parts:\n",
      "\"The Supreme Court on Friday overturned \", \"Roe V. Wade\", \" effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow, limit, or ban the practice altogether.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  5.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0 {'class_id': 0, 'class_label': 'negative', 'class_prob': 0.9391740560531616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# also manually specifying target entities (Fox article)\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "# Load the tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize sentiment classifier\n",
    "tsc = TargetSentimentClassifier()\n",
    "\n",
    "def process_text_with_manual_target(text, manual_targets=[\"Roe v Wade\", \"Roe V Wade\", \"Roe v. Wade\", \"Roe V. Wade\", \"roe v wade\"]):\n",
    "    # Perform named entity recognition\n",
    "    ner_spans = nlp(text)\n",
    "    \n",
    "    # Extract entities and print the list\n",
    "    ents = [span[\"word\"] for span in ner_spans]\n",
    "    print(f\"Entities: {ents}\")\n",
    "    \n",
    "    # Initialize positions, result list for quoted text parts, and structured data\n",
    "    last_end = 0\n",
    "    quoted_text_parts = []\n",
    "    data_for_analysis = []\n",
    "\n",
    "    # Check for manually specified target variations\n",
    "    found_target = None\n",
    "    for target in manual_targets:\n",
    "        target_index = text.find(target)\n",
    "        if target_index != -1:\n",
    "            found_target = (target, target_index)\n",
    "            break  # Stop at the first found variation\n",
    "\n",
    "    if found_target:\n",
    "        manual_target, target_index = found_target\n",
    "        # Get context around the manually specified target\n",
    "        pre_target_text = text[:target_index]\n",
    "        post_target_text = text[target_index + len(manual_target):]\n",
    "\n",
    "        # Append to the data for analysis with manual target\n",
    "        data_for_analysis.append((pre_target_text, manual_target, post_target_text))\n",
    "\n",
    "        # Also add it to quoted text parts for consistent output\n",
    "        quoted_text_parts.append(f'\"{pre_target_text}\"')\n",
    "        quoted_text_parts.append(f'\"{manual_target}\"')\n",
    "        last_end = target_index + len(manual_target)\n",
    "\n",
    "        # Print sentiment analysis for manual target\n",
    "        sentiment = tsc.infer_from_text(pre_target_text, manual_target, post_target_text)\n",
    "        print(f\"{manual_target}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{manual_target}\")\n",
    "\n",
    "    # Process automated NER spans\n",
    "    for span in ner_spans:\n",
    "        start, end = span['start'], span['end']\n",
    "\n",
    "        # Skip the manual target if it overlaps with NER spans\n",
    "        if start < last_end:\n",
    "            continue\n",
    "\n",
    "        # Segment the text before the entity\n",
    "        pre_entity_text = text[last_end:start]\n",
    "        \n",
    "        # Entity text\n",
    "        entity_text = text[start:end]\n",
    "        \n",
    "        # Text after the entity up to the next segment\n",
    "        post_entity_text = text[end:]\n",
    "        \n",
    "        # Add to quoted text parts for the final output format\n",
    "        quoted_text_parts.append(f'\"{pre_entity_text}\"')\n",
    "        quoted_text_parts.append(f'\"{entity_text}\"')\n",
    "\n",
    "        # Update data_for_analysis with the tuple\n",
    "        data_for_analysis.append((pre_entity_text, entity_text, post_entity_text))\n",
    "\n",
    "        # Print the entity sentiment information\n",
    "        sentiment = tsc.infer_from_text(pre_entity_text, entity_text, post_entity_text)\n",
    "        print(f\"{span['entity']}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{entity_text}\")\n",
    "\n",
    "        # Update last_end position to continue processing the rest of the text\n",
    "        last_end = end\n",
    "\n",
    "    # Final segment after the last entity (if any) and add closing quotes\n",
    "    if last_end < len(text):\n",
    "        quoted_text_parts.append(f'\"{text[last_end:]}\"')\n",
    "    \n",
    "    # Join the quoted text parts\n",
    "    quoted_text = ', '.join(quoted_text_parts)\n",
    "    print(\"Modified Text with Quoted Parts:\")\n",
    "    print(quoted_text)\n",
    "\n",
    "    return data_for_analysis  # Return structured data for analysis\n",
    "\n",
    "# Define a sample text containing the manual target \"Roe v Wade\" in different forms\n",
    "TEXT = \"\"\"The Supreme Court on Friday overturned Roe V. Wade effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow, limit, or ban the practice altogether.\"\"\"\n",
    "\n",
    "# Run the function on the sample text and save the structured data\n",
    "data_for_analysis = process_text_with_manual_target(TEXT)\n",
    "\n",
    "# Run sentiment analysis on the prepared data\n",
    "sentiments = tsc.infer(targets=data_for_analysis)\n",
    "\n",
    "# Display the sentiment results\n",
    "for i, result in enumerate(sentiments):\n",
    "    print(\"Sentiment: \", i, result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbcf2bc3-8665-4414-9f67-216bc765bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: ['Joe', 'B', '##iden', 'Supreme', 'Court', 'Roe', 'v', '.', 'Wade']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  5.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roe v. Wade\tnegative\t0.73\tRoe v. Wade\n",
      "Modified Text with Quoted Parts:\n",
      "\"President Joe Biden said Friday “the health and life of women in this nation are now at risk” after the Supreme Court overturned \", \"Roe v. Wade\", \" and eliminated the constitutional right to an abortion.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00,  5.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0 {'class_id': 0, 'class_label': 'negative', 'class_prob': 0.7293685078620911}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# also manually specifying target entities (CNN article)\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "# Load the tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize sentiment classifier\n",
    "tsc = TargetSentimentClassifier()\n",
    "\n",
    "def process_text_with_manual_target(text, manual_targets=[\"Roe v Wade\", \"Roe V Wade\", \"Roe v. Wade\", \"Roe V. Wade\", \"roe v wade\"]):\n",
    "    # Perform named entity recognition\n",
    "    ner_spans = nlp(text)\n",
    "    \n",
    "    # Extract entities and print the list\n",
    "    ents = [span[\"word\"] for span in ner_spans]\n",
    "    print(f\"Entities: {ents}\")\n",
    "    \n",
    "    # Initialize positions, result list for quoted text parts, and structured data\n",
    "    last_end = 0\n",
    "    quoted_text_parts = []\n",
    "    data_for_analysis = []\n",
    "\n",
    "    # Check for manually specified target variations\n",
    "    found_target = None\n",
    "    for target in manual_targets:\n",
    "        target_index = text.find(target)\n",
    "        if target_index != -1:\n",
    "            found_target = (target, target_index)\n",
    "            break  # Stop at the first found variation\n",
    "\n",
    "    if found_target:\n",
    "        manual_target, target_index = found_target\n",
    "        # Get context around the manually specified target\n",
    "        pre_target_text = text[:target_index]\n",
    "        post_target_text = text[target_index + len(manual_target):]\n",
    "\n",
    "        # Append to the data for analysis with manual target\n",
    "        data_for_analysis.append((pre_target_text, manual_target, post_target_text))\n",
    "\n",
    "        # Also add it to quoted text parts for consistent output\n",
    "        quoted_text_parts.append(f'\"{pre_target_text}\"')\n",
    "        quoted_text_parts.append(f'\"{manual_target}\"')\n",
    "        last_end = target_index + len(manual_target)\n",
    "\n",
    "        # Print sentiment analysis for manual target\n",
    "        sentiment = tsc.infer_from_text(pre_target_text, manual_target, post_target_text)\n",
    "        print(f\"{manual_target}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{manual_target}\")\n",
    "\n",
    "    # Process automated NER spans\n",
    "    for span in ner_spans:\n",
    "        start, end = span['start'], span['end']\n",
    "\n",
    "        # Skip the manual target if it overlaps with NER spans\n",
    "        if start < last_end:\n",
    "            continue\n",
    "\n",
    "        # Segment the text before the entity\n",
    "        pre_entity_text = text[last_end:start]\n",
    "        \n",
    "        # Entity text\n",
    "        entity_text = text[start:end]\n",
    "        \n",
    "        # Text after the entity up to the next segment\n",
    "        post_entity_text = text[end:]\n",
    "        \n",
    "        # Add to quoted text parts for the final output format\n",
    "        quoted_text_parts.append(f'\"{pre_entity_text}\"')\n",
    "        quoted_text_parts.append(f'\"{entity_text}\"')\n",
    "\n",
    "        # Update data_for_analysis with the tuple\n",
    "        data_for_analysis.append((pre_entity_text, entity_text, post_entity_text))\n",
    "\n",
    "        # Print the entity sentiment information\n",
    "        sentiment = tsc.infer_from_text(pre_entity_text, entity_text, post_entity_text)\n",
    "        print(f\"{span['entity']}\\t{sentiment[0]['class_label']}\\t{sentiment[0]['class_prob']:.2f}\\t{entity_text}\")\n",
    "\n",
    "        # Update last_end position to continue processing the rest of the text\n",
    "        last_end = end\n",
    "\n",
    "    # Final segment after the last entity (if any) and add closing quotes\n",
    "    if last_end < len(text):\n",
    "        quoted_text_parts.append(f'\"{text[last_end:]}\"')\n",
    "    \n",
    "    # Join the quoted text parts\n",
    "    quoted_text = ', '.join(quoted_text_parts)\n",
    "    print(\"Modified Text with Quoted Parts:\")\n",
    "    print(quoted_text)\n",
    "\n",
    "    return data_for_analysis  # Return structured data for analysis\n",
    "\n",
    "# Define a sample text containing the manual target \"Roe v Wade\" in different forms\n",
    "TEXT = \"\"\"President Joe Biden said Friday “the health and life of women in this nation are now at risk” after the Supreme Court overturned Roe v. Wade and eliminated the constitutional right to an abortion.\"\"\"\n",
    "\n",
    "# Run the function on the sample text and save the structured data\n",
    "data_for_analysis = process_text_with_manual_target(TEXT)\n",
    "\n",
    "# Run sentiment analysis on the prepared data\n",
    "sentiments = tsc.infer(targets=data_for_analysis)\n",
    "\n",
    "# Display the sentiment results\n",
    "for i, result in enumerate(sentiments):\n",
    "    print(\"Sentiment: \", i, result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63ec94-06b4-4c4f-a4f9-4296931bc6fe",
   "metadata": {},
   "source": [
    "## SieBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29930cde-f56f-4a77-b103-1dd5b6e28ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9975256323814392}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a552689-046b-4498-9cd8-177eb19de618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.998649537563324}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analysis(\"Around 73 million induced abortions take place worldwide each year Six out of 10 61 percent of all unintended pregnancies and 3 out of 10 29 percent of all pregnancies end in induced abortion Comprehensive abortion care is included in the list of essential health care services published by WHO in 2020 Abortion is a simple health care intervention that can be safely and effectively managed by a wide range of health workers using medication or a surgical procedure In the first 12 weeks of pregnancy a medical abortion can also be safely self managed by the pregnant person outside of a health care facility e g at home in whole or in part This requires that the woman has access to accurate information quality medicines and support from a trained health worker if she needs or wants it during the process Comprehensive abortion care includes the provision of information abortion management and post abortion care It encompasses care related to miscarriage spontaneous abortion and missed abortion induced abortion the deliberate interruption of an ongoing pregnancy by medical or surgical means incomplete abortion as well as intrauterine fetal demise The information in this fact sheet focuses on care related to induced abortion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cf76040-ec6c-4e6d-8225-243cb8d0ac4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9272783398628235}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fox article\n",
    "text = \"The Supreme Court on Friday overturned Roe v Wade effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow limit or ban the practice altogether The ruling came in the courts opinion in Dobbs v Jackson Womens Health Organization which centered on a Mississippi law that banned abortion after 15 weeks of pregnancy The Republican led state of Mississippi asked the Supreme Court to strike down a lower court ruling that stopped the 15 week abortion ban from taking place We end this opinion where we began Abortion presents a profound moral question The Constitution does not prohibit the citizens of each State from regulating or prohibiting abortion Roe and Casey arrogated that authority We now overrule those decisions and return that authority to the people and their elected representatives Justice Samuel Alito wrote in the courts opinion Alitos opinion began with an exploration and criticism of Roe v Wade and its holding that while states have a legitimate interest in protecting potential life this interest was not strong enough to prohibit abortions before the time of fetal viability understood to be at about 23 weeks into pregnancy\"\n",
    "\n",
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "320d4f16-296b-4c1c-b437-6b0a380ec31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9981071949005127}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN article\n",
    "text = \"The Supreme Court overturned Roe v Wade on Friday holding that there is no longer a federal constitutional right to an abortion The opinion is the most consequential Supreme Court decision in decades and will transform the landscape of womens reproductive health in America Going forward abortion rights will be determined by states unless Congress acts Already nearly half of the states have or will pass laws that ban abortion while others have enacted strict measures regulating the procedure Roe was egregiously wrong from the start Justice Samuel Alito wrote in his majority opinion Its reasoning was exceptionally weak and the decision has had damaging consequences And far from bringing about a national settlement of the abortion issue Roe and Casey have enflamed debate and deepened division The vote was 5 4 in favor of overturning Roe In a joint dissenting opinion Justices Stephen Breyer Sonia Sotomayor and Elena Kagan heavily criticized the majority closing With sorrow for this Court but more for the many millions of American women who have today lost a fundamental constitutional protection we dissent The opinion represents the culmination of a decades long effort on the part of critics of abortion seeking to return more power to the states It was made possible by a solid six member conservative majority including three of Donald Trumps nominees At least 21 states have laws or constitutional amendments already in place that would make them certain to attempt to ban abortion as quickly as possible according to the Guttmacher Institute which favors abortion rights And an additional four states are likely to ban abortions as soon as possible without federal protections Chief Justice John Roberts did not join the majority writing in a concurring opinion that he would not have overturned Roe but instead would have only uphold Mississippis law banning abortions after 15 weeks\"\n",
    "\n",
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dd713e6-ac57-4067-9e06-ca43ef3ca820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9971659779548645}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fox article shorter\n",
    "text = \"\"\"The Supreme Court on Friday overturned Roe V. Wade effectively ending recognition of a constitutional right to abortion and giving individual states the power to allow, limit, or ban the practice altogether. \"\"\"\n",
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "200a3ddd-33d6-4087-ae41-1880185a550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9900760054588318}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fox article shorter\n",
    "text = \"\"\"President Joe Biden said Friday “the health and life of women in this nation are now at risk” after the Supreme Court overturned Roe v. Wade and eliminated the constitutional right to an abortion.\"\"\"\n",
    "\n",
    "sentiment_analysis(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e50dcc5-d5c9-4ef6-8796-38d8704df8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9968834519386292}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am neither angry nor sad about this\"\n",
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdec176f-e6bb-4cf6-bf8b-0c56347f0920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9984942674636841}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am neither happy nor unhappy about this\"\n",
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53e59c83-6fde-4b02-b79a-0b0c83e772a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting nltk>=3.8 (from textblob)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.66.2)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk, textblob\n",
      "Successfully installed nltk-3.9.1 textblob-0.18.0.post0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Sentence: 'I am neither angry nor sad about this'\n",
      "Sentiment Score: -0.5\n",
      "Overall Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# trying TextBlob for comparison of complexity and performance\n",
    "%pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sentence to analyze\n",
    "sentence = \"I am neither angry nor sad about this\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "# Get the sentiment polarity\n",
    "sentiment_score = blob.sentiment.polarity\n",
    "\n",
    "# Determine the sentiment based on polarity\n",
    "if sentiment_score > 0:\n",
    "    sentiment = \"Positive\"\n",
    "elif sentiment_score < 0:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "# Display results\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Sentiment Score: {sentiment_score}\")\n",
    "print(f\"Overall Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cea33-b477-4552-b71f-3afa99a56049",
   "metadata": {},
   "source": [
    "# Issues with the different models\n",
    "## * SieBERT is only positive and negative, and always super confident. BUT understands \"I am neither angry nor sad about this\" as pocitive tho\n",
    "## * NewsSentiment only does sentiment on targets and thus requires segmentation to identify targets, so in practice it might not be super powerful for long articles (especially because it has a token limit). BUT could be justified to check out \"Roe v Wade\", \"abortion\" etc. + deems \"I am neither angry nor sad about this\" as neutral\n",
    "## * TextBlob isn't super good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fe0cc-ed19-48c7-81b4-0e65fd68d8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
